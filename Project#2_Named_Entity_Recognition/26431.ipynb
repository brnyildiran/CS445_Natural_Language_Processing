{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2thwmPOc2WK9"
      },
      "source": [
        "##Initialize Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWvnWFHt_TqY",
        "outputId": "325efb44-e318-41a7-ad85-a604d3cef9a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn<0.24\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (3.1.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.23.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.64.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 7.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.8 sklearn-crfsuite-0.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U 'scikit-learn<0.24'\n",
        "!pip install sklearn-crfsuite\n",
        "!pip install tqdm\n",
        "# YOU NEED TO RESTART THE RUNTIME!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZteNYk03GPL",
        "outputId": "e9a25801-23aa-489a-caa8-013ed120db7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to mount your drive to this notebook in order to read the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Du79PuQ7-MD8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOuZQf_I-PIq"
      },
      "source": [
        "## Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YF7EABYQ-JNt"
      },
      "outputs": [],
      "source": [
        "# Put the folder path where the datasets are located\n",
        "PATH = \"drive/My Drive/Colab Notebooks/CS445_NLP/Project #2/Datasets/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "43FBxXHoV2rW"
      },
      "outputs": [],
      "source": [
        "def read_data(data):\n",
        "  temp = []\n",
        "  with open(data) as f:\n",
        "        text = f.read()\n",
        "  paragraph_list = text.split('\\n\\n')\n",
        "  for sent in paragraph_list:\n",
        "    if sent != \"-DOCSTART- -X- -X- O\":\n",
        "      if sent != \"\":\n",
        "        items = sent.split('\\n')\n",
        "        result = tuple(tuple(i.split(' ')) for i in items)\n",
        "        temp.append(result)\n",
        "        #print(tuple(tuple(i.split(' ')) for i in items))\n",
        "  return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VWRYqesM38U",
        "outputId": "a87a263d-d2c9-46ff-ba93-783f126526e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('EU', 'NNP', 'B-NP', 'B-ORG'),\n",
              "  ('rejects', 'VBZ', 'B-VP', 'O'),\n",
              "  ('German', 'JJ', 'B-NP', 'B-MISC'),\n",
              "  ('call', 'NN', 'I-NP', 'O'),\n",
              "  ('to', 'TO', 'B-VP', 'O'),\n",
              "  ('boycott', 'VB', 'I-VP', 'O'),\n",
              "  ('British', 'JJ', 'B-NP', 'B-MISC'),\n",
              "  ('lamb', 'NN', 'I-NP', 'O'),\n",
              "  ('.', '.', 'O', 'O')),\n",
              " (('Peter', 'NNP', 'B-NP', 'B-PER'), ('Blackburn', 'NNP', 'I-NP', 'I-PER')),\n",
              " (('BRUSSELS', 'NNP', 'B-NP', 'B-LOC'), ('1996-08-22', 'CD', 'I-NP', 'O'))]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "final = read_data(PATH+\"train.txt\")\n",
        "final[:3]\n",
        "#DOSSTARTLARI at"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pgPCBhbj-dSJ"
      },
      "outputs": [],
      "source": [
        "# read data with your custom function\n",
        "train_data = read_data(PATH+\"train.txt\")\n",
        "val_data = read_data(PATH+\"valid.txt\")\n",
        "test_data = read_data(PATH+\"test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFs0zr2X03Eh",
        "outputId": "0dbafd57-8df6-4c7f-eea5-7fe3c7fe9aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the train data:  14041\n",
            "Length of the validation data:  3250\n",
            "Length of the test data:  3453\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of the train data: \", len(train_data))\n",
        "print(\"Length of the validation data: \", len(val_data))\n",
        "print(\"Length of the test data: \", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIx8Urj3r9U_",
        "outputId": "2775c1cf-9b0e-48ed-c68e-bb223c1a9230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('EU', 'NNP', 'B-NP', 'B-ORG'), ('rejects', 'VBZ', 'B-VP', 'O'), ('German', 'JJ', 'B-NP', 'B-MISC'), ('call', 'NN', 'I-NP', 'O'), ('to', 'TO', 'B-VP', 'O'), ('boycott', 'VB', 'I-VP', 'O'), ('British', 'JJ', 'B-NP', 'B-MISC'), ('lamb', 'NN', 'I-NP', 'O'), ('.', '.', 'O', 'O')), (('Peter', 'NNP', 'B-NP', 'B-PER'), ('Blackburn', 'NNP', 'I-NP', 'I-PER')), (('BRUSSELS', 'NNP', 'B-NP', 'B-LOC'), ('1996-08-22', 'CD', 'I-NP', 'O'))]\n"
          ]
        }
      ],
      "source": [
        "# Train data\n",
        "print(train_data[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqSiFQNgs77m",
        "outputId": "2249900d-0b9f-4906-8b0c-6850591d816d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('CRICKET', 'NNP', 'B-NP', 'O'), ('-', ':', 'O', 'O'), ('LEICESTERSHIRE', 'NNP', 'B-NP', 'B-ORG'), ('TAKE', 'NNP', 'I-NP', 'O'), ('OVER', 'IN', 'B-PP', 'O'), ('AT', 'NNP', 'B-NP', 'O'), ('TOP', 'NNP', 'I-NP', 'O'), ('AFTER', 'NNP', 'I-NP', 'O'), ('INNINGS', 'NNP', 'I-NP', 'O'), ('VICTORY', 'NN', 'I-NP', 'O'), ('.', '.', 'O', 'O')), (('LONDON', 'NNP', 'B-NP', 'B-LOC'), ('1996-08-30', 'CD', 'I-NP', 'O')), (('West', 'NNP', 'B-NP', 'B-MISC'), ('Indian', 'NNP', 'I-NP', 'I-MISC'), ('all-rounder', 'NN', 'I-NP', 'O'), ('Phil', 'NNP', 'I-NP', 'B-PER'), ('Simmons', 'NNP', 'I-NP', 'I-PER'), ('took', 'VBD', 'B-VP', 'O'), ('four', 'CD', 'B-NP', 'O'), ('for', 'IN', 'B-PP', 'O'), ('38', 'CD', 'B-NP', 'O'), ('on', 'IN', 'B-PP', 'O'), ('Friday', 'NNP', 'B-NP', 'O'), ('as', 'IN', 'B-PP', 'O'), ('Leicestershire', 'NNP', 'B-NP', 'B-ORG'), ('beat', 'VBD', 'B-VP', 'O'), ('Somerset', 'NNP', 'B-NP', 'B-ORG'), ('by', 'IN', 'B-PP', 'O'), ('an', 'DT', 'B-NP', 'O'), ('innings', 'NN', 'I-NP', 'O'), ('and', 'CC', 'O', 'O'), ('39', 'CD', 'B-NP', 'O'), ('runs', 'NNS', 'I-NP', 'O'), ('in', 'IN', 'B-PP', 'O'), ('two', 'CD', 'B-NP', 'O'), ('days', 'NNS', 'I-NP', 'O'), ('to', 'TO', 'B-VP', 'O'), ('take', 'VB', 'I-VP', 'O'), ('over', 'IN', 'B-PP', 'O'), ('at', 'IN', 'B-PP', 'O'), ('the', 'DT', 'B-NP', 'O'), ('head', 'NN', 'I-NP', 'O'), ('of', 'IN', 'B-PP', 'O'), ('the', 'DT', 'B-NP', 'O'), ('county', 'NN', 'I-NP', 'O'), ('championship', 'NN', 'I-NP', 'O'), ('.', '.', 'O', 'O')), (('Their', 'PRP$', 'B-NP', 'O'), ('stay', 'NN', 'I-NP', 'O'), ('on', 'IN', 'B-PP', 'O'), ('top', 'NN', 'B-NP', 'O'), (',', ',', 'O', 'O'), ('though', 'RB', 'B-ADVP', 'O'), (',', ',', 'O', 'O'), ('may', 'MD', 'B-VP', 'O'), ('be', 'VB', 'I-VP', 'O'), ('short-lived', 'JJ', 'B-ADJP', 'O'), ('as', 'IN', 'B-PP', 'O'), ('title', 'NN', 'B-NP', 'O'), ('rivals', 'NNS', 'I-NP', 'O'), ('Essex', 'NNP', 'I-NP', 'B-ORG'), (',', ',', 'O', 'O'), ('Derbyshire', 'NNP', 'B-NP', 'B-ORG'), ('and', 'CC', 'I-NP', 'O'), ('Surrey', 'NNP', 'I-NP', 'B-ORG'), ('all', 'DT', 'O', 'O'), ('closed', 'VBD', 'B-VP', 'O'), ('in', 'RP', 'B-PRT', 'O'), ('on', 'IN', 'B-PP', 'O'), ('victory', 'NN', 'B-NP', 'O'), ('while', 'IN', 'B-SBAR', 'O'), ('Kent', 'NNP', 'B-NP', 'B-ORG'), ('made', 'VBD', 'B-VP', 'O'), ('up', 'RP', 'B-PRT', 'O'), ('for', 'IN', 'B-PP', 'O'), ('lost', 'VBN', 'B-NP', 'O'), ('time', 'NN', 'I-NP', 'O'), ('in', 'IN', 'B-PP', 'O'), ('their', 'PRP$', 'B-NP', 'O'), ('rain-affected', 'JJ', 'I-NP', 'O'), ('match', 'NN', 'I-NP', 'O'), ('against', 'IN', 'B-PP', 'O'), ('Nottinghamshire', 'NNP', 'B-NP', 'B-ORG'), ('.', '.', 'O', 'O')), (('After', 'IN', 'B-PP', 'O'), ('bowling', 'VBG', 'B-NP', 'O'), ('Somerset', 'NNP', 'I-NP', 'B-ORG'), ('out', 'RP', 'B-PRT', 'O'), ('for', 'IN', 'B-PP', 'O'), ('83', 'CD', 'B-NP', 'O'), ('on', 'IN', 'B-PP', 'O'), ('the', 'DT', 'B-NP', 'O'), ('opening', 'NN', 'I-NP', 'O'), ('morning', 'NN', 'I-NP', 'O'), ('at', 'IN', 'B-PP', 'O'), ('Grace', 'NNP', 'B-NP', 'B-LOC'), ('Road', 'NNP', 'I-NP', 'I-LOC'), (',', ',', 'O', 'O'), ('Leicestershire', 'NNP', 'B-NP', 'B-ORG'), ('extended', 'VBD', 'B-VP', 'O'), ('their', 'PRP$', 'B-NP', 'O'), ('first', 'JJ', 'I-NP', 'O'), ('innings', 'NN', 'I-NP', 'O'), ('by', 'IN', 'B-PP', 'O'), ('94', 'CD', 'B-NP', 'O'), ('runs', 'VBZ', 'B-VP', 'O'), ('before', 'IN', 'B-PP', 'O'), ('being', 'VBG', 'B-VP', 'O'), ('bowled', 'VBD', 'I-VP', 'O'), ('out', 'RP', 'B-PRT', 'O'), ('for', 'IN', 'B-PP', 'O'), ('296', 'CD', 'B-NP', 'O'), ('with', 'IN', 'B-PP', 'O'), ('England', 'NNP', 'B-NP', 'B-LOC'), ('discard', 'VBP', 'B-VP', 'O'), ('Andy', 'NNP', 'B-NP', 'B-PER'), ('Caddick', 'NNP', 'I-NP', 'I-PER'), ('taking', 'VBG', 'B-VP', 'O'), ('three', 'CD', 'B-NP', 'O'), ('for', 'IN', 'B-PP', 'O'), ('83', 'CD', 'B-NP', 'O'), ('.', '.', 'O', 'O'))]\n"
          ]
        }
      ],
      "source": [
        "# Validation data\n",
        "print(val_data[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av6drqjTs_uQ",
        "outputId": "963f22cf-c60d-4cb4-a5be-76c9a9ea98ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('SOCCER', 'NN', 'B-NP', 'O'), ('-', ':', 'O', 'O'), ('JAPAN', 'NNP', 'B-NP', 'B-LOC'), ('GET', 'VB', 'B-VP', 'O'), ('LUCKY', 'NNP', 'B-NP', 'O'), ('WIN', 'NNP', 'I-NP', 'O'), (',', ',', 'O', 'O'), ('CHINA', 'NNP', 'B-NP', 'B-PER'), ('IN', 'IN', 'B-PP', 'O'), ('SURPRISE', 'DT', 'B-NP', 'O'), ('DEFEAT', 'NN', 'I-NP', 'O'), ('.', '.', 'O', 'O')), (('Nadim', 'NNP', 'B-NP', 'B-PER'), ('Ladki', 'NNP', 'I-NP', 'I-PER')), (('AL-AIN', 'NNP', 'B-NP', 'B-LOC'), (',', ',', 'O', 'O'), ('United', 'NNP', 'B-NP', 'B-LOC'), ('Arab', 'NNP', 'I-NP', 'I-LOC'), ('Emirates', 'NNPS', 'I-NP', 'I-LOC'), ('1996-12-06', 'CD', 'I-NP', 'O')), (('Japan', 'NNP', 'B-NP', 'B-LOC'), ('began', 'VBD', 'B-VP', 'O'), ('the', 'DT', 'B-NP', 'O'), ('defence', 'NN', 'I-NP', 'O'), ('of', 'IN', 'B-PP', 'O'), ('their', 'PRP$', 'B-NP', 'O'), ('Asian', 'JJ', 'I-NP', 'B-MISC'), ('Cup', 'NNP', 'I-NP', 'I-MISC'), ('title', 'NN', 'I-NP', 'O'), ('with', 'IN', 'B-PP', 'O'), ('a', 'DT', 'B-NP', 'O'), ('lucky', 'JJ', 'I-NP', 'O'), ('2-1', 'CD', 'I-NP', 'O'), ('win', 'VBP', 'B-VP', 'O'), ('against', 'IN', 'B-PP', 'O'), ('Syria', 'NNP', 'B-NP', 'B-LOC'), ('in', 'IN', 'B-PP', 'O'), ('a', 'DT', 'B-NP', 'O'), ('Group', 'NNP', 'I-NP', 'O'), ('C', 'NNP', 'I-NP', 'O'), ('championship', 'NN', 'I-NP', 'O'), ('match', 'NN', 'I-NP', 'O'), ('on', 'IN', 'B-PP', 'O'), ('Friday', 'NNP', 'B-NP', 'O'), ('.', '.', 'O', 'O')), (('But', 'CC', 'O', 'O'), ('China', 'NNP', 'B-NP', 'B-LOC'), ('saw', 'VBD', 'B-VP', 'O'), ('their', 'PRP$', 'B-NP', 'O'), ('luck', 'NN', 'I-NP', 'O'), ('desert', 'VB', 'B-VP', 'O'), ('them', 'PRP', 'B-NP', 'O'), ('in', 'IN', 'B-PP', 'O'), ('the', 'DT', 'B-NP', 'O'), ('second', 'NN', 'I-NP', 'O'), ('match', 'NN', 'I-NP', 'O'), ('of', 'IN', 'B-PP', 'O'), ('the', 'DT', 'B-NP', 'O'), ('group', 'NN', 'I-NP', 'O'), (',', ',', 'O', 'O'), ('crashing', 'VBG', 'B-VP', 'O'), ('to', 'TO', 'B-PP', 'O'), ('a', 'DT', 'B-NP', 'O'), ('surprise', 'NN', 'I-NP', 'O'), ('2-0', 'CD', 'I-NP', 'O'), ('defeat', 'NN', 'I-NP', 'O'), ('to', 'TO', 'B-PP', 'O'), ('newcomers', 'NNS', 'B-NP', 'O'), ('Uzbekistan', 'NNP', 'I-NP', 'B-LOC'), ('.', '.', 'O', 'O'))]\n"
          ]
        }
      ],
      "source": [
        "# Test Data\n",
        "print(test_data[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECYsXDBl-7mx"
      },
      "source": [
        "# Create Gazetteer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KDHV6sAxnRsT"
      },
      "outputs": [],
      "source": [
        "def href_regex_extractor(text):\n",
        "  regex = re.compile(\"href=\\\\\\\"(?:%\\d+)?((?:([A-ZIÜŞÖÇ][A-Za-zığüşöç]*))(?:%\\d+)?)*\\\\\\\"&gt;(\\s?[A-ZIÜŞÖÇ][A-Za-zığüşöç]*\\s?)*&lt;\")\n",
        "  temp = []\n",
        "  for m in re.finditer(regex, text): \n",
        "    temp.append(m.group())\n",
        "  return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lOeeeEvgX7B1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b30d2ee0-4239-49d8-9e99-fac43dba1283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1129/20000 [00:33<00:29, 639.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "This file cannot be processed:\n",
            "<_io.TextIOWrapper name='/content/drive/My Drive/Colab Notebooks/CS445_NLP/Project #2/Wikipedia_Pages/wikipedia_pages/wikipedia_pages/37138.json' mode='r' encoding='UTF-8'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20000/20000 [01:06<00:00, 299.80it/s]\n"
          ]
        }
      ],
      "source": [
        "# Load wikipedia pages and extract the href expressions to a list.\n",
        "from tqdm import tqdm\n",
        "\n",
        "dir = \"/content/drive/My Drive/Colab Notebooks/CS445_NLP/Project #2/Wikipedia_Pages/wikipedia_pages/wikipedia_pages\"\n",
        "\n",
        "hrefs = []\n",
        "count = 0\n",
        "for file in tqdm(os.listdir(dir)):\n",
        "  with open(dir+ '/' + file, 'r') as file:\n",
        "    if count == 999:\n",
        "      print(\"\\n\\nThis file cannot be processed:\")\n",
        "      print(file)\n",
        "      count+=1\n",
        "    else:\n",
        "      data = json.load(file)\n",
        "      result = href_regex_extractor(data.get('text'))\n",
        "      result = \",\".join(result)\n",
        "      hrefs.append(result)\n",
        "      count+=1\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ko3TxdE2QIQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e8b9565-3cb1-4f01-f62c-3d7659bb11b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19999\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['href=\"Stanley%20Kramer\"&gt;Stanley Kramer&lt;,href=\"Marlon%20Brando\"&gt;Marlon Brando&lt;,href=\"American%20Motorcyclist%20Association\"&gt;American Motorcyclist Association&lt;,href=\"VHS\"&gt;VHS&lt;,href=\"Betamax\"&gt;Betamax&lt;,href=\"DVD\"&gt;DVD&lt;,href=\"Sony%20Pictures\"&gt;Sony Pictures&lt;,href=\"Mill%20Creek%20Entertainment\"&gt;Mill Creek Entertainment&lt;,href=\"Rotten%20Tomatoes\"&gt;Rotten Tomatoes&lt;,href=\"Dave%20Kehr\"&gt;Dave Kehr&lt;,href=\"Chicago%20Reader\"&gt;Chicago Reader&lt;,href=\"Paddington\"&gt;Paddington&lt;,href=\"Triumph%20Engineering\"&gt;Triumph&lt;,href=\"Gil%20Stratton\"&gt;Gil Stratton Jr&lt;,href=\"American%20Film%20Institute\"&gt;American Film Institute&lt;,href=\"James%20Dean\"&gt;James Dean&lt;,href=\"Elvis%20Presley\"&gt;Elvis Presley&lt;,href=\"The%20Beatles%20Anthology\"&gt;The Beatles Anthology&lt;,href=\"Black%20Rebel%20Motorcycle%20Club\"&gt;Black Rebel Motorcycle Club&lt;,href=\"Everybody%20Loves%20Raymond\"&gt;Everybody Loves Raymond&lt;,href=\"The%20Simpsons\"&gt;The Simpsons&lt;,href=\"Separate%20Vocations\"&gt;Separate Vocations&lt;,href=\"Michael%20Cera\"&gt;Michael Cera&lt;',\n",
              " 'href=\"Slateford\"&gt;Hutchison&lt;,href=\"Tommy%20Younger\"&gt;Tommy Younger&lt;,href=\"Fluminense%20Football%20Club\"&gt;Fluminense&lt;,href=\"European%20Cup\"&gt;European Cup&lt;,href=\"Stade%20Reims\"&gt;Stade Reims&lt;,href=\"Raymond%20Kopa\"&gt;Raymond Kopa&lt;,href=\"Tommy%20Preston\"&gt;Tommy Preston&lt;,href=\"William%20Harrower\"&gt;William Harrower&lt;,href=\"Bob%20Shankly\"&gt;Bob Shankly&lt;,href=\"Bill%20Shankly\"&gt;Bill Shankly&lt;,href=\"Erich%20Schaedler\"&gt;Erich Schaedler&lt;,href=\"John%20Brownlie\"&gt;John Brownlie&lt;,href=\"Peter%20Marinello\"&gt;Peter Marinello&lt;',\n",
              " 'href=\"Northwest%20Highlands\"&gt;Northwest Highlands&lt;,href=\"Scotland\"&gt;Scotland&lt;,href=\"Loch%20Mullardoch\"&gt;Loch Mullardoch&lt;,href=\"Glen%20Cannich\"&gt;Glen Cannich&lt;,href=\"Glen%20Strathfarrar\"&gt;Glen Strathfarrar&lt;,href=\"Great%20Glen\"&gt;Great Glen&lt;,href=\"Moray%20Firth\"&gt;Moray Firth&lt;,href=\"Mam%20Sodhail\"&gt;Mam Sodhail&lt;,href=\"Munro\"&gt;Munro&lt;,href=\"An%20Riabhachan\"&gt;An Riabhachan&lt;,href=\"Ordnance%20Survey\"&gt;Ordnance Survey&lt;',\n",
              " 'href=\"Major%20League%20Baseball\"&gt;Major Leagues&lt;,href=\"Chicago%20Cubs\"&gt;Chicago Cubs&lt;,href=\"Chicago%20White%20Sox\"&gt;Chicago White Sox&lt;,href=\"Philadelphia%20Phillies\"&gt;Philadelphia Phillies&lt;,href=\"Pompton%20Lakes%20High%20School\"&gt;Pompton Lakes High School&lt;,href=\"National%20League\"&gt;National League&lt;,href=\"New%20York%20Yankees\"&gt;New York Yankees&lt;,href=\"North%20Atlantic%20League\"&gt;North Atlantic League&lt;,href=\"Southern%20Association\"&gt;Southern Association&lt;,href=\"Arkansas%20Travelers\"&gt;Arkansas Travelers&lt;,href=\"Texas%20League\"&gt;Texas League&lt;,href=\"Little%20Rock%2C%20Arkansas\"&gt;Little Rock&lt;',\n",
              " 'href=\"Deputy%20Lieutenant\"&gt;DL&lt;,href=\"Thames%20Ditton\"&gt;Thames Ditton&lt;,href=\"Surrey\"&gt;Surrey&lt;,href=\"Lord%20Henry%20FitzGerald\"&gt;Lord Henry FitzGerald&lt;,href=\"Lord%20Edward%20FitzGerald\"&gt;Lord Edward FitzGerald&lt;,href=\"Black%20Sea\"&gt;Black Sea&lt;,href=\"Gentleman%20Usher\"&gt;Gentleman Usher&lt;,href=\"Queen%20Victoria\"&gt;Queen Victoria&lt;,href=\"Deputy%20Lieutenant\"&gt;Deputy Lieutenant&lt;,href=\"Crimean%20War\"&gt;Crimean War&lt;,href=\"Crimea\"&gt;Crimea&lt;,href=\"Molecombe\"&gt;Molecombe&lt;,href=\"Sussex\"&gt;Sussex&lt;,href=\"London\"&gt;London&lt;,href=\"London\"&gt;London&lt;,href=\"Strangford\"&gt;Strangford&lt;,href=\"County%20Down\"&gt;County Down&lt;,href=\"London\"&gt;London&lt;',\n",
              " 'href=\"Angola\"&gt;Angola&lt;',\n",
              " 'href=\"Computer%20Numeric%20Control\"&gt;Computer Numeric Control&lt;',\n",
              " 'href=\"Trinidad\"&gt;Trinidad&lt;,href=\"Essex%20Community%20College\"&gt;Essex Community College&lt;',\n",
              " 'href=\"Discovery%20Channel%20Canada\"&gt;Discovery Channel Canada&lt;,href=\"Channel%20Five\"&gt;Channel Five&lt;,href=\"William%20Shatner\"&gt;William Shatner&lt;,href=\"Star%20Trek\"&gt;Star Trek&lt;,href=\"Gene%20Roddenberry\"&gt;Gene Roddenberry&lt;,href=\"Motorola\"&gt;Motorola&lt;,href=\"Seth%20Shostak\"&gt;Seth Shostak&lt;,href=\"SETI\"&gt;SETI&lt;,href=\"Amok%20Time\"&gt;Amok Time&lt;,href=\"Bill%20Gates\"&gt;Bill Gates&lt;,href=\"BASIC\"&gt;BASIC&lt;,href=\"Microsoft\"&gt;Microsoft&lt;,href=\"Miguel%20Alcubierre\"&gt;Miguel Alcubierre&lt;,href=\"Kevin%20Warwick\"&gt;Kevin Warwick&lt;,href=\"Steve%20Perlman\"&gt;Steve Perlman&lt;,href=\"QuickTime\"&gt;QuickTime&lt;,href=\"Vancouver\"&gt;Vancouver&lt;,href=\"Emmy%20Awards\"&gt;Emmy Awards&lt;',\n",
              " 'href=\"Ukraine\"&gt;Ukraine&lt;,href=\"Valeriy%20Sydorenko\"&gt;Valeriy Sydorenko&lt;']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "print(len(hrefs))\n",
        "hrefs[:10]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GrHbVDcSarVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece2727e-7a9e-4cbc-df76-f97a26d64c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19999/19999 [00:03<00:00, 6221.50it/s]\n"
          ]
        }
      ],
      "source": [
        "# REGEX Extract Title\n",
        "def regex_extract_title(text):\n",
        "  regex = re.compile(\"\\\"((?:%\\d+)?[A-ZIÜŞÖÇ0-9][A-Za-zığüşöç0-9]*)*\")\n",
        "  temp = []\n",
        "  for m in re.finditer(regex, text):\n",
        "\n",
        "    regex_1 = re.compile(\"\\\"\")\n",
        "    subst_1 = \"\"\n",
        "    result_1 = re.sub(regex_1, subst_1, m.group(0), 0)\n",
        "\n",
        "    regex_2 = re.compile(\"(%\\d+)\")\n",
        "    subst_2 = \" \"\n",
        "    result_2 = re.sub(regex_2, subst_2, result_1, 0)\n",
        "    temp.append(result_2)\n",
        "\n",
        "  return temp\n",
        "  \n",
        "\n",
        "\n",
        "count_row = 0\n",
        "extracted_titles = []\n",
        "\n",
        "for row in tqdm(hrefs):\n",
        "  #print(i)\n",
        "  result_regex = regex_extract_title(row)\n",
        "  for i in result_regex:\n",
        "    if i  != '':\n",
        "      extracted_titles.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IqQjM7RRiZYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1938f0-9e21-4c30-c7f0-212df6710bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19999/19999 [00:01<00:00, 19911.71it/s]\n"
          ]
        }
      ],
      "source": [
        "#REGEX Extract Anchor\n",
        "def regex_extract_anchor(text):\n",
        "  regex = re.compile(\"gt;([A-ZIÜŞÖÇ0-9][A-Za-zığüşöç0-9]*( )?)*\")\n",
        "  temp = []\n",
        "  for m in re.finditer(regex, text):\n",
        "    regex_2 = re.compile(\"gt;\")\n",
        "    subst_2 = \"\"\n",
        "    result = re.sub(regex_2, subst_2, m.group(0), 0)\n",
        "    temp.append(result)\n",
        "  return temp\n",
        "\n",
        "\n",
        "count_row_anchor = 0\n",
        "extracted_anchors = []\n",
        "\n",
        "for row in tqdm(hrefs):\n",
        "  result_regex_anchor = regex_extract_anchor(row)\n",
        "  for i in result_regex_anchor:\n",
        "    extracted_anchors.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eQYVUfnKlJoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb6ef95-2f35-4fe6-ca5f-d6b1d0e1d764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of the extracted titles:  331842\n",
            "Sample Extracted Titles:\n",
            "\n",
            "['Chicago Reader', 'Paddington', 'Triumph Engineering', 'Gil Stratton', 'American Film Institute', 'James Dean', 'Elvis Presley', 'The Beatles Anthology']\n",
            "\n",
            "\n",
            "\n",
            "Amount of the extracted anchors:  331842\n",
            "Sample ExtractedAnchors:\n",
            "\n",
            "['Chicago Reader', 'Paddington', 'Triumph', 'Gil Stratton Jr', 'American Film Institute', 'James Dean', 'Elvis Presley', 'The Beatles Anthology']\n"
          ]
        }
      ],
      "source": [
        "# Extraction Results from JSON Files\n",
        "# Titles\n",
        "print(\"Amount of the extracted titles: \", len(extracted_titles))\n",
        "print(\"Sample Extracted Titles:\\n\")\n",
        "print(extracted_titles[10:18])\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "print(\"\\n\\n\")\n",
        "# Anchors\n",
        "print(\"Amount of the extracted anchors: \", len(extracted_anchors))\n",
        "print(\"Sample ExtractedAnchors:\\n\")\n",
        "print(extracted_anchors[10:18])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YT4ntyeKPKL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26437fe9-59b5-4a52-b3b8-63cae0b29751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the Gazetteer after adding titles and anchors:  663684\n",
            "Unique Words in the Gazetteer:  165610\n",
            "Gazetteer:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Stanley Kramer',\n",
              " 'Marlon Brando',\n",
              " 'American Motorcyclist Association',\n",
              " 'VHS',\n",
              " 'Betamax',\n",
              " 'DVD',\n",
              " 'Sony Pictures',\n",
              " 'Mill Creek Entertainment',\n",
              " 'Rotten Tomatoes',\n",
              " 'Dave Kehr',\n",
              " 'Chicago Reader',\n",
              " 'Paddington',\n",
              " 'Triumph Engineering',\n",
              " 'Gil Stratton',\n",
              " 'American Film Institute',\n",
              " 'James Dean',\n",
              " 'Elvis Presley',\n",
              " 'The Beatles Anthology',\n",
              " 'Black Rebel Motorcycle Club',\n",
              " 'Everybody Loves Raymond']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Create Gazetteer Word List with all unique words\n",
        "MyGazetteer = []\n",
        "\n",
        "# Append titles and anchors to the Gazetteer\n",
        "MyGazetteer = extracted_titles + extracted_anchors\n",
        "print(\"Size of the Gazetteer after adding titles and anchors: \", len(MyGazetteer))\n",
        "\n",
        "# Remove the dublicates from MyGazetteer\n",
        "UniqueGazetteer = {}\n",
        "\n",
        "for i in MyGazetteer:\n",
        "  UniqueGazetteer[i] = \"\"\n",
        "\n",
        "print(\"Unique Words in the Gazetteer: \", len(UniqueGazetteer.keys()))\n",
        "print(\"Gazetteer:\")\n",
        "MyGazetteer[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hbhw-VC_Bni"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INZH7AlE_Fnw"
      },
      "source": [
        "## Conditional Random Fields (CRF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ac67uI_Ity"
      },
      "source": [
        "### Extract features for CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UKh8kvu6_Dze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a42632-6b3e-4d67-fcbc-a9451eaf9bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "C8-QXZAl-WeN"
      },
      "outputs": [],
      "source": [
        "def wordShape(text):\n",
        "    t1 = re.sub('[A-Z]', 'X',text)\n",
        "    t2 = re.sub('[a-z]', 'x', t1)\n",
        "    return re.sub('[0-9]', 'd', t2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VF1pRr3z_3J0"
      },
      "outputs": [],
      "source": [
        "def shortWordShape(text):\n",
        "    t1 = re.sub('([A-Z]+)', 'X',text)\n",
        "    t2 = re.sub('([a-z]+)', 'x', t1)\n",
        "    return re.sub('([0-9]+)', 'd', t2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pTJruh7NBcPc"
      },
      "outputs": [],
      "source": [
        "def has_number(text):\n",
        "    return bool(re.search(r'\\d', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uaeabUoyCxHz"
      },
      "outputs": [],
      "source": [
        "def has_hyphen(text):\n",
        "    return bool(re.search(r'-', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wAevZvm2GKv8"
      },
      "outputs": [],
      "source": [
        "def is_upper_has_digit_has_dash(text):\n",
        "  result = False\n",
        "  if text.isupper():\n",
        "    if has_number(text):\n",
        "      if bool(re.search(r'-', text)):\n",
        "        result = True\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jgVUOD06TLGT"
      },
      "outputs": [],
      "source": [
        "def is_stopword(text):\n",
        "  result = False\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  for word in stop_words:\n",
        "    if text == word:\n",
        "      result = True\n",
        "    else:\n",
        "      result = False\n",
        "      \n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-NHdX7qZY3SZ"
      },
      "outputs": [],
      "source": [
        "def is_in_gazetteer(text):\n",
        "  for word in MyGazetteer:\n",
        "    result = False\n",
        "    if text == word:\n",
        "      return True\n",
        "    else:\n",
        "      result = False\n",
        "      \n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "y_8lrN7Z_bHz"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus.reader.wordnet import WordNetError\n",
        "# create a function to extract features for each token\n",
        "\n",
        "def token2features(sentence: list, idx: int) -> dict:\n",
        "  ps = PorterStemmer()\n",
        "  word = sentence[idx][0]\n",
        "  features = {\n",
        "      'stem': \"None\",\n",
        "      'POS_tag': \"None\",\n",
        "      'Chunk_tag': \"None\",\n",
        "      'is_BOS': \"None\",\n",
        "      'is_EOS': \"None\",\n",
        "      'is_first_letter_uppercase': \"None\",\n",
        "      'word_shape': \"None\",\n",
        "      'short_word_shape': \"None\",\n",
        "      'has_number': \"None\",\n",
        "      'has_hyphen': \"None\",\n",
        "      'is_upper_has_digit_has_dash': \"None\",\n",
        "      'prefix':\"None\",\n",
        "      'suffix': \"None\",\n",
        "      'is_all_upper': \"None\",\n",
        "      'is_stopword': \"None\",\n",
        "\n",
        "      # Neighbor Words\n",
        "      # Word Before\n",
        "      'word_previous': \"None\",\n",
        "      'word_previous_shape': \"None\",\n",
        "      'word_previous_short_shape': \"None\",\n",
        "\n",
        "      # Word After\n",
        "      'word_after': \"None\",\n",
        "      'word_after_shape': \"None\",\n",
        "      'word_after_short_shape': \"None\",\n",
        "\n",
        "      'is_in_gazetteer':\"None\"\n",
        "  }\n",
        "\n",
        "  features.update({'stem' : ps.stem(word)}),\n",
        "  features.update({'POS_tag' : sentence[idx][1]})\n",
        "  features.update({'Chunk_tag' : sentence[idx][2]})\n",
        "\n",
        "  if(len(sentence) <= 2):\n",
        "    features.update({\n",
        "        'is_BOS' : True,\n",
        "        'is_EOS' : True\n",
        "    })\n",
        "  \n",
        "\n",
        "  else:\n",
        "    # FIRST WORD\n",
        "    if idx == 0:\n",
        "      features.update({\n",
        "          'is_BOS' : True,\n",
        "          'is_EOS' : False,\n",
        "          'word_after' : sentence[idx+1][0],\n",
        "          'word_after_shape' : wordShape(sentence[idx+1][0]),\n",
        "          'word_after_short_shape' : shortWordShape(sentence[idx+1][0]) \n",
        "      })\n",
        "        \n",
        "\n",
        "    # LAST WORD\n",
        "    elif idx == len(sentence)-1:\n",
        "      features.update({\n",
        "          'is_BOS' : False,\n",
        "          'is_EOS' : True,\n",
        "          'word_previous' : sentence[idx-1][0],\n",
        "          'word_previous_shape' : wordShape(sentence[idx-1][0]),\n",
        "          'word_previous_short_shape' : shortWordShape(sentence[idx-1][0])\n",
        "      })\n",
        "\n",
        "\n",
        "    else:\n",
        "      #  WORD IN THE MIDDLE\n",
        "      features.update({\n",
        "          'is_BOS' : False,\n",
        "          'is_EOS' : False,\n",
        "          'word_previous' : sentence[idx-1][0],\n",
        "          'word_after' : sentence[idx+1][0],\n",
        "\n",
        "          'word_previous_shape' : wordShape(sentence[idx-1][0]),\n",
        "          'word_previous_short_shape' : shortWordShape(sentence[idx-1][0]),\n",
        "\n",
        "          'word_after_shape' : wordShape(sentence[idx+1][0]),\n",
        "          'word_after_short_shape' : shortWordShape(sentence[idx+1][0])\n",
        "          \n",
        "      })\n",
        "      \n",
        "  features.update({\n",
        "      'is_first_letter_uppercase' : word[0].isupper(),\n",
        "      'word_shape' : wordShape(word),\n",
        "      'short_word_shape' : shortWordShape(word),\n",
        "      'has_number' : has_number(word),\n",
        "      'has_hyphen' : has_hyphen(word),\n",
        "      'is_upper_has_digit_has_dash' : is_upper_has_digit_has_dash(word),\n",
        "      'prefix' : word[:4],\n",
        "      'suffix' : word[-4:],\n",
        "      'is_all_upper' : word.isupper(),\n",
        "      'is_stopword' : is_stopword(word),\n",
        "      'is_in_gazetteer' : is_in_gazetteer(word)\n",
        "  })\n",
        "  return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "v57POpXU_bK8"
      },
      "outputs": [],
      "source": [
        "# define function to process each token given a sentence\n",
        "def sent2features(sentence: list) -> list:\n",
        "  return [token2features(sentence, i) for i in range(len(sentence))]\n",
        "\n",
        "# get named entity labels from the sentence\n",
        "def sent2labels(sentence: list) -> list:\n",
        "  NER_labels = []\n",
        "  for word in sentence:\n",
        "    NER_labels.append(word[3])\n",
        "  return NER_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5ta1NN3b_vkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132b4f59-f96d-4465-be4a-7f508e843815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14041/14041 [2:00:22<00:00,  1.94it/s]\n",
            "100%|██████████| 3250/3250 [31:07<00:00,  1.74it/s]\n",
            "100%|██████████| 3453/3453 [27:43<00:00,  2.08it/s]\n"
          ]
        }
      ],
      "source": [
        "# prepare inputs and labels\n",
        "\n",
        "train_sents = [sent2features(s) for s in tqdm(train_data)]\n",
        "val_sents = [sent2features(s) for s in tqdm(val_data)]\n",
        "test_sents = [sent2features(s) for s in tqdm(test_data)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uG7beW6DNiNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6349337-7484-4d05-883c-ab99a264bb80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14041/14041 [00:00<00:00, 366432.03it/s]\n",
            "100%|██████████| 3250/3250 [00:00<00:00, 274098.93it/s]\n",
            "100%|██████████| 3453/3453 [00:00<00:00, 241048.74it/s]\n"
          ]
        }
      ],
      "source": [
        "train_labels = [sent2labels(s) for s in tqdm(train_data)]\n",
        "val_labels = [sent2labels(s) for s in tqdm(val_data)]\n",
        "test_labels = [sent2labels(s) for s in tqdm(test_data)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BaXTG6tfFBKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803e47a8-93db-4c5d-8c3f-b6f8dc4dadd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Chunk_tag': 'B-NP',\n",
              "  'POS_tag': 'NNP',\n",
              "  'has_hyphen': False,\n",
              "  'has_number': False,\n",
              "  'is_BOS': True,\n",
              "  'is_EOS': True,\n",
              "  'is_all_upper': False,\n",
              "  'is_first_letter_uppercase': True,\n",
              "  'is_in_gazetteer': True,\n",
              "  'is_stopword': False,\n",
              "  'is_upper_has_digit_has_dash': False,\n",
              "  'prefix': 'Pete',\n",
              "  'short_word_shape': 'Xx',\n",
              "  'stem': 'peter',\n",
              "  'suffix': 'eter',\n",
              "  'word_after': 'None',\n",
              "  'word_after_shape': 'None',\n",
              "  'word_after_short_shape': 'None',\n",
              "  'word_previous': 'None',\n",
              "  'word_previous_shape': 'None',\n",
              "  'word_previous_short_shape': 'None',\n",
              "  'word_shape': 'Xxxxx'},\n",
              " {'Chunk_tag': 'I-NP',\n",
              "  'POS_tag': 'NNP',\n",
              "  'has_hyphen': False,\n",
              "  'has_number': False,\n",
              "  'is_BOS': True,\n",
              "  'is_EOS': True,\n",
              "  'is_all_upper': False,\n",
              "  'is_first_letter_uppercase': True,\n",
              "  'is_in_gazetteer': True,\n",
              "  'is_stopword': False,\n",
              "  'is_upper_has_digit_has_dash': False,\n",
              "  'prefix': 'Blac',\n",
              "  'short_word_shape': 'Xx',\n",
              "  'stem': 'blackburn',\n",
              "  'suffix': 'burn',\n",
              "  'word_after': 'None',\n",
              "  'word_after_shape': 'None',\n",
              "  'word_after_short_shape': 'None',\n",
              "  'word_previous': 'None',\n",
              "  'word_previous_shape': 'None',\n",
              "  'word_previous_short_shape': 'None',\n",
              "  'word_shape': 'Xxxxxxxxx'}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "train_sents[1] # Peter Blackburn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "oTuNgV60JQ1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a0a489-198f-48f7-def1-6756828559bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-PER', 'I-PER']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "train_labels[1] # Peter Blackburn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6WgW13i842Y"
      },
      "source": [
        "###Initialize the CRF Model and GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6mPO9W_pV14H"
      },
      "outputs": [],
      "source": [
        "# Initialize the CRF model\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "m_XPjjvLAWHV"
      },
      "outputs": [],
      "source": [
        "# Set the hyperparameter space that will be scanned.\n",
        "# TAKES SO MUCH TIME, THEREFORE I AM COMMENTING THIS PART\n",
        "#crf= sklearn_crfsuite.CRF()\n",
        "\n",
        "#grid_params_CRF = {\n",
        "#  \"c1\": [0, 0.05, 0.25],\n",
        "#  \"c2\": [0, 0.05, 0.25]\n",
        "#}\n",
        "\n",
        "# initialize GridSearchCV for CRF\n",
        "#grid_search_CRF = GridSearchCV(\n",
        "#                 estimator = crf,\n",
        "#                 param_grid = grid_params_CRF,\n",
        "#                 verbose = 2)\n",
        "\n",
        "\n",
        "# fitting the model for grid search \n",
        "#grid_search_CRF.fit(train_sents, train_labels)\n",
        "\n",
        "\n",
        "#print best parameter after tuning \n",
        "#print(\"Best Parameters: \", grid_search_CRF.best_params_)\n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning \n",
        "#print(\"Model after hyper-parameter tuning: \", grid_search_CRF.cv_results_['params'][grid_search_CRF.best_index_])\n",
        "#print(\"Best Estimator: \", grid_search_CRF.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "KgfYVqnBA1lA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adbf5206-29f1-442a-85a2-46dcd24a9a33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision Score: 0.9788677529727192 \n",
            " Recall Score: 0.9791480082551303 \n",
            " F1 Score: 0.9789197238702536\n"
          ]
        }
      ],
      "source": [
        "# initialize and train a crf model with best hyper-parameters\n",
        "crf_best_estimator = sklearn_crfsuite.CRF(c1=0.05, c2=0.25, keep_tempfiles=None)\n",
        "\n",
        "# fit your model\n",
        "crf_best_estimator.fit(train_sents, train_labels)\n",
        "\n",
        "# make predictions\n",
        "preds_crf = crf_best_estimator.predict(val_sents)\n",
        "\n",
        "# calculate f1-score and classification report for test using sklearn_crfsuite.metrics class\n",
        "# evaluate on validation set\n",
        "precision_crf = sklearn_crfsuite.metrics.flat_precision_score(val_labels, preds_crf, average = 'weighted')\n",
        "recall_crf = sklearn_crfsuite.metrics.flat_recall_score(val_labels, preds_crf, average = 'weighted')\n",
        "f1_crf = sklearn_crfsuite.metrics.flat_f1_score(val_labels, preds_crf, average = 'weighted')\n",
        "\n",
        "print(\"Precision Score:\", precision_crf, \"\\n\",\n",
        "      \"Recall Score:\", recall_crf, \"\\n\",\n",
        "      \"F1 Score:\", f1_crf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "aP2NrUHIzLzT"
      },
      "outputs": [],
      "source": [
        "def drop_dict_keys(sent_list, remove_features_list):\n",
        "  for k in remove_features_list:\n",
        "    for sentence in sent_list:\n",
        "      for word in sentence:\n",
        "        word.pop(k, None)\n",
        "  return sent_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "i68-YwT56AkI"
      },
      "outputs": [],
      "source": [
        "def get_crf_results(train_sents_p, val_sents_p, train_labels_, val_labels_):\n",
        "  crf_best_estimator = sklearn_crfsuite.CRF(c1=0.05, c2=0.25, keep_tempfiles=None)\n",
        "  crf_best_estimator.fit(train_sents_p, train_labels_)\n",
        "  crf_best_estimator_preds = crf_best_estimator.predict(val_sents_p)\n",
        "\n",
        "  precisions_crf = sklearn_crfsuite.metrics.flat_precision_score(val_labels_, crf_best_estimator_preds, average = 'weighted')\n",
        "  recalls_crf = sklearn_crfsuite.metrics.flat_recall_score(val_labels_, crf_best_estimator_preds, average = 'weighted')\n",
        "  f1_scores_crf = sklearn_crfsuite.metrics.flat_f1_score(val_labels_, crf_best_estimator_preds, average = 'weighted')\n",
        "\n",
        "  #results_add_feature_1by1_df = pd.DataFrame({\"Features\": features_for_add_1by1, \"Precision\": precisions_1by1, \"Recall\": recalls_1by1, \"F1 Score\": f1_scores_1by1})\n",
        "  result = [precisions_crf, recalls_crf, f1_scores_crf]\n",
        "  print(result)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Give Features One By One to the CRF Model"
      ],
      "metadata": {
        "id": "FuznehOk4yIm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "oLfwRD_On_Yk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e50b85f-4165-4ad0-b91a-7a0e015f39c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.930163099892689, 0.9332775203457809, 0.9288045819990084]\n",
            "[0.9580974997587028, 0.9588216969744169, 0.9578701400004289]\n",
            "[0.9604951150014482, 0.9606713134223746, 0.9601071505128342]\n",
            "[0.9600043134996975, 0.9603403294264242, 0.9597384972039107]\n",
            "[0.9602500652615021, 0.9606323741287333, 0.9599931748925127]\n",
            "[0.9675709307350736, 0.9669794789922511, 0.966903021502621]\n",
            "[0.9693719917466549, 0.9694910634321093, 0.9692645097456983]\n",
            "[0.9691546919098003, 0.969257427670262, 0.9690595203734476]\n",
            "[0.9692768673217568, 0.9693158366107238, 0.9691387041066284]\n",
            "[0.9692937027752458, 0.9693353062575445, 0.9691537641514449]\n",
            "[0.9691918653839233, 0.969257427670262, 0.9690714504498199]\n",
            "[0.971676373929788, 0.9718468906974027, 0.9716730303301021]\n",
            "[0.9723147521160607, 0.9726062069234064, 0.972400137126605]\n",
            "[0.9723838702581897, 0.9726646158638682, 0.9724634936188514]\n",
            "[0.972330372673442, 0.972625676570227, 0.9724165451660205]\n",
            "[0.9754232897023084, 0.9757602897083447, 0.975528555960034]\n",
            "[0.9765209019662725, 0.9768116506366574, 0.976606597389527]\n",
            "[0.9764651961041952, 0.9767727113430162, 0.9765530341449151]\n",
            "[0.9776788955126534, 0.9780187687395351, 0.9777792848857483]\n",
            "[0.978659701278296, 0.9789922510805654, 0.9787434212932529]\n",
            "[0.9788914180328855, 0.9792258868424126, 0.9789724200059373]\n",
            "[0.9788677529727192, 0.9791480082551303, 0.9789197238702536]\n"
          ]
        }
      ],
      "source": [
        "#'stem'\n",
        "import copy\n",
        "features_remove_1 = ( \n",
        "            'POS_tag', 'Chunk_tag', 'is_BOS', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',\n",
        "            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_1 = copy.deepcopy(train_sents)\n",
        "val_sents_1 = copy.deepcopy(val_sents)\n",
        "train_sents_1 = drop_dict_keys(train_sents_1, features_remove_1)\n",
        "val_sents_1 = drop_dict_keys(val_sents_1, features_remove_1)\n",
        "result_1 = get_crf_results(train_sents_1, val_sents_1, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag'\n",
        "features_remove_2 = ('Chunk_tag', 'is_BOS', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',\n",
        "            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_2 = copy.deepcopy(train_sents)\n",
        "val_sents_2 = copy.deepcopy(val_sents)\n",
        "train_sents_2 = drop_dict_keys(train_sents_2, features_remove_2)\n",
        "val_sents_2 = drop_dict_keys(val_sents_2, features_remove_2)\n",
        "result_2 = get_crf_results(train_sents_2, val_sents_2, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', \n",
        "features_remove_3 = ( 'is_BOS', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',\n",
        "            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_3 = copy.deepcopy(train_sents)\n",
        "val_sents_3 = copy.deepcopy(val_sents)\n",
        "train_sents_3 = drop_dict_keys(train_sents_3, features_remove_3)\n",
        "val_sents_3 = drop_dict_keys(val_sents_3, features_remove_3)\n",
        "result_3 = get_crf_results(train_sents_3, val_sents_3, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_BOS',\n",
        "features_remove_4 = ( 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',\n",
        "            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_4 = copy.deepcopy(train_sents)\n",
        "val_sents_4 = copy.deepcopy(val_sents)\n",
        "train_sents_4 = drop_dict_keys(train_sents_4, features_remove_4)\n",
        "val_sents_4 = drop_dict_keys(val_sents_4, features_remove_4)\n",
        "result_4 = get_crf_results(train_sents_4, val_sents_4, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS',\n",
        "features_remove_5 = ( 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',\n",
        "            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_5 = copy.deepcopy(train_sents)\n",
        "val_sents_5 = copy.deepcopy(val_sents)\n",
        "train_sents_5 = drop_dict_keys(train_sents_5, features_remove_5)\n",
        "val_sents_5 = drop_dict_keys(val_sents_5, features_remove_5)\n",
        "result_5 = get_crf_results(train_sents_5, val_sents_5, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase',\n",
        "features_remove_6 = ('word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',\n",
        "            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_6 = copy.deepcopy(train_sents)\n",
        "val_sents_6 = copy.deepcopy(val_sents)\n",
        "train_sents_6 = drop_dict_keys(train_sents_6, features_remove_6)\n",
        "val_sents_6 = drop_dict_keys(val_sents_6, features_remove_6)\n",
        "result_6 = get_crf_results(train_sents_6, val_sents_6, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', \n",
        "features_remove_7 = ('short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',\n",
        "            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_7 = copy.deepcopy(train_sents)\n",
        "val_sents_7 = copy.deepcopy(val_sents)\n",
        "train_sents_7 = drop_dict_keys(train_sents_7, features_remove_7)\n",
        "val_sents_7 = drop_dict_keys(val_sents_7, features_remove_7)\n",
        "result_7 = get_crf_results(train_sents_7, val_sents_7, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape',\n",
        "features_remove_8 = ('has_number','has_hyphen','is_upper_has_digit_has_dash',\n",
        "            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_8 = copy.deepcopy(train_sents)\n",
        "val_sents_8 = copy.deepcopy(val_sents)\n",
        "train_sents_8 = drop_dict_keys(train_sents_8, features_remove_8)\n",
        "val_sents_8 = drop_dict_keys(val_sents_8, features_remove_8)\n",
        "result_8 = get_crf_results(train_sents_8, val_sents_8, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number',\n",
        "features_remove_9 = ('has_hyphen','is_upper_has_digit_has_dash',\n",
        "            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_9 = copy.deepcopy(train_sents)\n",
        "val_sents_9 = copy.deepcopy(val_sents)\n",
        "train_sents_9 = drop_dict_keys(train_sents_9, features_remove_9)\n",
        "val_sents_9 = drop_dict_keys(val_sents_9, features_remove_9)\n",
        "result_9 = get_crf_results(train_sents_9, val_sents_9, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen',\n",
        "features_remove_10 = ('is_upper_has_digit_has_dash',\n",
        "            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_10 = copy.deepcopy(train_sents)\n",
        "val_sents_10 = copy.deepcopy(val_sents)\n",
        "train_sents_10 = drop_dict_keys(train_sents_10, features_remove_10)\n",
        "val_sents_10 = drop_dict_keys(val_sents_10, features_remove_10)\n",
        "result_10 = get_crf_results(train_sents_10, val_sents_10, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash',\n",
        "features_remove_11 = ('prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_11 = copy.deepcopy(train_sents)\n",
        "val_sents_11 = copy.deepcopy(val_sents)\n",
        "train_sents_11 = drop_dict_keys(train_sents_11, features_remove_11)\n",
        "val_sents_11 = drop_dict_keys(val_sents_11, features_remove_11)\n",
        "result_11 = get_crf_results(train_sents_11, val_sents_11, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', \n",
        "features_remove_12 = ('suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_12 = copy.deepcopy(train_sents)\n",
        "val_sents_12 = copy.deepcopy(val_sents)\n",
        "train_sents_12 = drop_dict_keys(train_sents_12, features_remove_12)\n",
        "val_sents_12 = drop_dict_keys(val_sents_12, features_remove_12)\n",
        "result_12 = get_crf_results(train_sents_12, val_sents_12, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix',\n",
        "features_remove_13 = ('is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_13 = copy.deepcopy(train_sents)\n",
        "val_sents_13 = copy.deepcopy(val_sents)\n",
        "train_sents_13 = drop_dict_keys(train_sents_13, features_remove_13)\n",
        "val_sents_13 = drop_dict_keys(val_sents_13, features_remove_13)\n",
        "result_13 = get_crf_results(train_sents_13, val_sents_13, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper',\n",
        "features_remove_14 = ('is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_14 = copy.deepcopy(train_sents)\n",
        "val_sents_14 = copy.deepcopy(val_sents)\n",
        "train_sents_14 = drop_dict_keys(train_sents_14, features_remove_14)\n",
        "val_sents_14 = drop_dict_keys(val_sents_14, features_remove_14)\n",
        "result_14 = get_crf_results(train_sents_14, val_sents_14, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',\n",
        "features_remove_15 = ('word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_15 = copy.deepcopy(train_sents)\n",
        "val_sents_15 = copy.deepcopy(val_sents)\n",
        "train_sents_15 = drop_dict_keys(train_sents_15, features_remove_15)\n",
        "val_sents_15 = drop_dict_keys(val_sents_15, features_remove_15)\n",
        "result_15 = get_crf_results(train_sents_15, val_sents_15, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',\n",
        "# 'word_previous', \n",
        "features_remove_16 = ('word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_16 = copy.deepcopy(train_sents)\n",
        "val_sents_16 = copy.deepcopy(val_sents)\n",
        "train_sents_16 = drop_dict_keys(train_sents_16, features_remove_16)\n",
        "val_sents_16 = drop_dict_keys(val_sents_16, features_remove_16)\n",
        "result_16 = get_crf_results(train_sents_16, val_sents_16, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',\n",
        "# 'word_previous', 'word_previous_shape',\n",
        "features_remove_17 = ('word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_17 = copy.deepcopy(train_sents)\n",
        "val_sents_17 = copy.deepcopy(val_sents)\n",
        "train_sents_17 = drop_dict_keys(train_sents_17, features_remove_17)\n",
        "val_sents_17 = drop_dict_keys(val_sents_17, features_remove_17)\n",
        "result_17 = get_crf_results(train_sents_17, val_sents_17, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',\n",
        "# 'word_previous', 'word_previous_shape', 'word_previous_short_shape',\n",
        "features_remove_18 = ('word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_18 = copy.deepcopy(train_sents)\n",
        "val_sents_18 = copy.deepcopy(val_sents)\n",
        "train_sents_18 = drop_dict_keys(train_sents_18, features_remove_18)\n",
        "val_sents_18 = drop_dict_keys(val_sents_18, features_remove_18)\n",
        "result_18 = get_crf_results(train_sents_18, val_sents_18, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',\n",
        "# 'word_previous', 'word_previous_shape', 'word_previous_short_shape', 'word_after',\n",
        "features_remove_19 = ('word_after_shape','word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_19 = copy.deepcopy(train_sents)\n",
        "val_sents_19 = copy.deepcopy(val_sents)\n",
        "train_sents_19 = drop_dict_keys(train_sents_19, features_remove_19)\n",
        "val_sents_19 = drop_dict_keys(val_sents_19, features_remove_19)\n",
        "result_19 = get_crf_results(train_sents_19, val_sents_19, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',\n",
        "# 'word_previous', 'word_previous_shape', 'word_previous_short_shape', 'word_after', 'word_after_shape',\n",
        "features_remove_20 = ('word_after_short_shape','is_in_gazetteer')\n",
        "train_sents_20 = copy.deepcopy(train_sents)\n",
        "val_sents_20 = copy.deepcopy(val_sents)\n",
        "train_sents_20 = drop_dict_keys(train_sents_20, features_remove_20)\n",
        "val_sents_20 = drop_dict_keys(val_sents_20, features_remove_20)\n",
        "result_20 = get_crf_results(train_sents_20, val_sents_20, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',\n",
        "# 'word_previous', 'word_previous_shape', 'word_previous_short_shape', 'word_after', 'word_after_shape', 'word_after_short_shape',\n",
        "features_remove_21 = ('is_in_gazetteer', 'is_in_gazetteer')\n",
        "train_sents_21 = copy.deepcopy(train_sents)\n",
        "val_sents_21 = copy.deepcopy(val_sents)\n",
        "train_sents_21 = drop_dict_keys(train_sents_21, features_remove_21)\n",
        "val_sents_21 = drop_dict_keys(val_sents_21, features_remove_21)\n",
        "result_21 = get_crf_results(train_sents_21, val_sents_21, train_labels, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',\n",
        "# 'word_previous', 'word_previous_shape', 'word_previous_short_shape', 'word_after', 'word_after_shape', 'word_after_short_shape', 'is_in_gazetteer'\n",
        "features_remove_22 = ()\n",
        "train_sents_22 = copy.deepcopy(train_sents)\n",
        "val_sents_22 = copy.deepcopy(val_sents)\n",
        "train_sents_22 = drop_dict_keys(train_sents_22, features_remove_22)\n",
        "val_sents_22 = drop_dict_keys(val_sents_22, features_remove_22)\n",
        "result_22 = get_crf_results(train_sents_22, val_sents_22, train_labels, val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6q2IwqA7WeiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78096372-1108-4236-ddaf-f6e8f80710b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'stem': 'eu'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False, 'prefix': 'EU'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False, 'prefix': 'EU', 'suffix': 'EU'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False, 'prefix': 'EU', 'suffix': 'EU', 'is_all_upper': True}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False, 'prefix': 'EU', 'suffix': 'EU', 'is_all_upper': True, 'is_stopword': False}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False, 'prefix': 'EU', 'suffix': 'EU', 'is_all_upper': True, 'is_stopword': False, 'word_previous': 'None'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False, 'prefix': 'EU', 'suffix': 'EU', 'is_all_upper': True, 'is_stopword': False, 'word_previous': 'None', 'word_previous_shape': 'None'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False, 'prefix': 'EU', 'suffix': 'EU', 'is_all_upper': True, 'is_stopword': False, 'word_previous': 'None', 'word_previous_shape': 'None', 'word_previous_short_shape': 'None'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False, 'prefix': 'EU', 'suffix': 'EU', 'is_all_upper': True, 'is_stopword': False, 'word_previous': 'None', 'word_previous_shape': 'None', 'word_previous_short_shape': 'None', 'word_after': 'rejects'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False, 'prefix': 'EU', 'suffix': 'EU', 'is_all_upper': True, 'is_stopword': False, 'word_previous': 'None', 'word_previous_shape': 'None', 'word_previous_short_shape': 'None', 'word_after': 'rejects', 'word_after_shape': 'xxxxxxx'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False, 'prefix': 'EU', 'suffix': 'EU', 'is_all_upper': True, 'is_stopword': False, 'word_previous': 'None', 'word_previous_shape': 'None', 'word_previous_short_shape': 'None', 'word_after': 'rejects', 'word_after_shape': 'xxxxxxx', 'word_after_short_shape': 'x'}\n",
            "{'stem': 'eu', 'POS_tag': 'NNP', 'Chunk_tag': 'B-NP', 'is_BOS': True, 'is_EOS': False, 'is_first_letter_uppercase': True, 'word_shape': 'XX', 'short_word_shape': 'X', 'has_number': False, 'has_hyphen': False, 'is_upper_has_digit_has_dash': False, 'prefix': 'EU', 'suffix': 'EU', 'is_all_upper': True, 'is_stopword': False, 'word_previous': 'None', 'word_previous_shape': 'None', 'word_previous_short_shape': 'None', 'word_after': 'rejects', 'word_after_shape': 'xxxxxxx', 'word_after_short_shape': 'x', 'is_in_gazetteer': True}\n"
          ]
        }
      ],
      "source": [
        "print(train_sents_1[0][0])\n",
        "print(train_sents_2[0][0])\n",
        "print(train_sents_3[0][0])\n",
        "print(train_sents_4[0][0])\n",
        "print(train_sents_5[0][0])\n",
        "print(train_sents_6[0][0])\n",
        "print(train_sents_7[0][0])\n",
        "print(train_sents_8[0][0])\n",
        "print(train_sents_9[0][0])\n",
        "print(train_sents_10[0][0])\n",
        "print(train_sents_11[0][0])\n",
        "print(train_sents_12[0][0])\n",
        "print(train_sents_13[0][0])\n",
        "print(train_sents_14[0][0])\n",
        "print(train_sents_15[0][0])\n",
        "print(train_sents_16[0][0])\n",
        "print(train_sents_17[0][0])\n",
        "print(train_sents_18[0][0])\n",
        "print(train_sents_19[0][0])\n",
        "print(train_sents_20[0][0])\n",
        "print(train_sents_21[0][0])\n",
        "print(train_sents_22[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "uH_mTKcmAoV3"
      },
      "outputs": [],
      "source": [
        "# start from the stem of the token and add features one by one and train a new model with each feature that you add\n",
        "\n",
        "\n",
        "features_for_add_1by1 = ['Stem', \n",
        "            '+POS_tag', \n",
        "            '+Chunk_tag', \n",
        "            '+is_BOS', \n",
        "            '+is_EOS', \n",
        "            '+is_first_letter_uppercase', \n",
        "            '+word_shape', \n",
        "            '+short_word_shape',\n",
        "            '+has_number',\n",
        "            '+has_hyphen',\n",
        "            '+is_upper_has_digit_has_dash',\n",
        "            '+prefix',\n",
        "            '+suffix',\n",
        "            '+is_all_upper',\n",
        "            '+is_stopword',\n",
        "            '+word_previous',\n",
        "            '+word_previous_shape',\n",
        "            '+word_previous_short_shape',\n",
        "            '+word_after',\n",
        "            '+word_after_shape',\n",
        "            '+word_after_short_shape',\n",
        "            '+is_in_gazetteer']\n",
        "\n",
        "\n",
        "\n",
        "results_features_1by1_df = pd.DataFrame(columns = [\"Precision\", \"Recall\", \"F1 Score\"])\n",
        "series_1 = pd.Series(result_1, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_1, ignore_index=True)\n",
        "\n",
        "series_2 = pd.Series(result_2, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_2, ignore_index=True)\n",
        "\n",
        "series_3 = pd.Series(result_3, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_3, ignore_index=True)\n",
        "\n",
        "series_4 = pd.Series(result_4, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_4, ignore_index=True)\n",
        "\n",
        "series_5 = pd.Series(result_5, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_5, ignore_index=True)\n",
        "\n",
        "series_6 = pd.Series(result_6, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_6, ignore_index=True)\n",
        "\n",
        "series_7 = pd.Series(result_7, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_7, ignore_index=True)\n",
        "\n",
        "series_8 = pd.Series(result_8, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_8, ignore_index=True)\n",
        "\n",
        "series_9 = pd.Series(result_9, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_9, ignore_index=True)\n",
        "\n",
        "series_10 = pd.Series(result_10, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_10, ignore_index=True)\n",
        "\n",
        "series_11 = pd.Series(result_11, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_11, ignore_index=True)\n",
        "\n",
        "series_12 = pd.Series(result_12, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_12, ignore_index=True)\n",
        "\n",
        "series_13 = pd.Series(result_13, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_13, ignore_index=True)\n",
        "\n",
        "series_14 = pd.Series(result_14, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_14, ignore_index=True)\n",
        "\n",
        "series_15 = pd.Series(result_15, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_15, ignore_index=True)\n",
        "\n",
        "series_16 = pd.Series(result_16, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_16, ignore_index=True)\n",
        "\n",
        "series_17 = pd.Series(result_17, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_17, ignore_index=True)\n",
        "\n",
        "series_18 = pd.Series(result_18, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_18, ignore_index=True)\n",
        "\n",
        "series_19 = pd.Series(result_19, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_19, ignore_index=True)\n",
        "\n",
        "series_20 = pd.Series(result_20, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_20, ignore_index=True)\n",
        "\n",
        "series_21 = pd.Series(result_21, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_21, ignore_index=True)\n",
        "\n",
        "series_22 = pd.Series(result_22, index = results_features_1by1_df.columns)\n",
        "results_features_1by1_df = results_features_1by1_df.append(series_22, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results Table of the Effect of Each Feature to the CRF Model"
      ],
      "metadata": {
        "id": "mMt9aGRU5Db9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "crVCdHIsTf_1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "outputId": "4483ff52-b519-4148-e120-fbf5c1357232"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        Features  Precision    Recall  F1 Score\n",
              "0                           Stem   0.930163  0.933278  0.928805\n",
              "1                       +POS_tag   0.958097  0.958822  0.957870\n",
              "2                     +Chunk_tag   0.960495  0.960671  0.960107\n",
              "3                        +is_BOS   0.960004  0.960340  0.959738\n",
              "4                        +is_EOS   0.960250  0.960632  0.959993\n",
              "5     +is_first_letter_uppercase   0.967571  0.966979  0.966903\n",
              "6                    +word_shape   0.969372  0.969491  0.969265\n",
              "7              +short_word_shape   0.969155  0.969257  0.969060\n",
              "8                    +has_number   0.969277  0.969316  0.969139\n",
              "9                    +has_hyphen   0.969294  0.969335  0.969154\n",
              "10  +is_upper_has_digit_has_dash   0.969192  0.969257  0.969071\n",
              "11                       +prefix   0.971676  0.971847  0.971673\n",
              "12                       +suffix   0.972315  0.972606  0.972400\n",
              "13                 +is_all_upper   0.972384  0.972665  0.972463\n",
              "14                  +is_stopword   0.972330  0.972626  0.972417\n",
              "15                +word_previous   0.975423  0.975760  0.975529\n",
              "16          +word_previous_shape   0.976521  0.976812  0.976607\n",
              "17    +word_previous_short_shape   0.976465  0.976773  0.976553\n",
              "18                   +word_after   0.977679  0.978019  0.977779\n",
              "19             +word_after_shape   0.978660  0.978992  0.978743\n",
              "20       +word_after_short_shape   0.978891  0.979226  0.978972\n",
              "21              +is_in_gazetteer   0.978868  0.979148  0.978920"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5505a4b5-06a8-4d2c-9802-4d782f5a81c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Features</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stem</td>\n",
              "      <td>0.930163</td>\n",
              "      <td>0.933278</td>\n",
              "      <td>0.928805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>+POS_tag</td>\n",
              "      <td>0.958097</td>\n",
              "      <td>0.958822</td>\n",
              "      <td>0.957870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>+Chunk_tag</td>\n",
              "      <td>0.960495</td>\n",
              "      <td>0.960671</td>\n",
              "      <td>0.960107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>+is_BOS</td>\n",
              "      <td>0.960004</td>\n",
              "      <td>0.960340</td>\n",
              "      <td>0.959738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>+is_EOS</td>\n",
              "      <td>0.960250</td>\n",
              "      <td>0.960632</td>\n",
              "      <td>0.959993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>+is_first_letter_uppercase</td>\n",
              "      <td>0.967571</td>\n",
              "      <td>0.966979</td>\n",
              "      <td>0.966903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>+word_shape</td>\n",
              "      <td>0.969372</td>\n",
              "      <td>0.969491</td>\n",
              "      <td>0.969265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>+short_word_shape</td>\n",
              "      <td>0.969155</td>\n",
              "      <td>0.969257</td>\n",
              "      <td>0.969060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>+has_number</td>\n",
              "      <td>0.969277</td>\n",
              "      <td>0.969316</td>\n",
              "      <td>0.969139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>+has_hyphen</td>\n",
              "      <td>0.969294</td>\n",
              "      <td>0.969335</td>\n",
              "      <td>0.969154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>+is_upper_has_digit_has_dash</td>\n",
              "      <td>0.969192</td>\n",
              "      <td>0.969257</td>\n",
              "      <td>0.969071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>+prefix</td>\n",
              "      <td>0.971676</td>\n",
              "      <td>0.971847</td>\n",
              "      <td>0.971673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>+suffix</td>\n",
              "      <td>0.972315</td>\n",
              "      <td>0.972606</td>\n",
              "      <td>0.972400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>+is_all_upper</td>\n",
              "      <td>0.972384</td>\n",
              "      <td>0.972665</td>\n",
              "      <td>0.972463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>+is_stopword</td>\n",
              "      <td>0.972330</td>\n",
              "      <td>0.972626</td>\n",
              "      <td>0.972417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>+word_previous</td>\n",
              "      <td>0.975423</td>\n",
              "      <td>0.975760</td>\n",
              "      <td>0.975529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>+word_previous_shape</td>\n",
              "      <td>0.976521</td>\n",
              "      <td>0.976812</td>\n",
              "      <td>0.976607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>+word_previous_short_shape</td>\n",
              "      <td>0.976465</td>\n",
              "      <td>0.976773</td>\n",
              "      <td>0.976553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>+word_after</td>\n",
              "      <td>0.977679</td>\n",
              "      <td>0.978019</td>\n",
              "      <td>0.977779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>+word_after_shape</td>\n",
              "      <td>0.978660</td>\n",
              "      <td>0.978992</td>\n",
              "      <td>0.978743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>+word_after_short_shape</td>\n",
              "      <td>0.978891</td>\n",
              "      <td>0.979226</td>\n",
              "      <td>0.978972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>+is_in_gazetteer</td>\n",
              "      <td>0.978868</td>\n",
              "      <td>0.979148</td>\n",
              "      <td>0.978920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5505a4b5-06a8-4d2c-9802-4d782f5a81c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5505a4b5-06a8-4d2c-9802-4d782f5a81c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5505a4b5-06a8-4d2c-9802-4d782f5a81c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# display the result table\n",
        "results_features_1by1_df.insert(0, \"Features\", features_for_add_1by1)\n",
        "results_features_1by1_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TvZ-Qr14BzZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c36df1af-8f95-48c0-e01b-2d66091d531a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature [Stem] overall result:  0.9307484007458261\n",
            "Feature [+POS_tag] overall result:  0.9582631122445161\n",
            "Feature [+Chunk_tag] overall result:  0.960424526312219\n",
            "Feature [+is_BOS] overall result:  0.9600277133766775\n",
            "Feature [+is_EOS] overall result:  0.9602918714275828\n",
            "Feature [+is_first_letter_uppercase] overall result:  0.9671511437433152\n",
            "Feature [+word_shape] overall result:  0.9693758549748209\n",
            "Feature [+short_word_shape] overall result:  0.9691572133178367\n",
            "Feature [+has_number] overall result:  0.9692438026797031\n",
            "Feature [+has_hyphen] overall result:  0.969260924394745\n",
            "Feature [+is_upper_has_digit_has_dash] overall result:  0.9691735811680017\n",
            "Feature [+prefix] overall result:  0.9717320983190976\n",
            "Feature [+suffix] overall result:  0.9724403653886907\n",
            "Feature [+is_all_upper] overall result:  0.9725039932469698\n",
            "Feature [+is_stopword] overall result:  0.9724575314698966\n",
            "Feature [+word_previous] overall result:  0.9755707117902291\n",
            "Feature [+word_previous_shape] overall result:  0.9766463833308189\n",
            "Feature [+word_previous_short_shape] overall result:  0.9765969805307089\n",
            "Feature [+word_after] overall result:  0.9778256497126456\n",
            "Feature [+word_after_shape] overall result:  0.9787984578840381\n",
            "Feature [+word_after_short_shape] overall result:  0.9790299082937451\n",
            "Feature [+is_in_gazetteer] overall result:  0.9789784950327011\n"
          ]
        }
      ],
      "source": [
        "# Average Precision - Recall - F1 Score for Features\n",
        "avg_result = 0\n",
        "feature = \"\"\n",
        "features_avg = []\n",
        "feature_names = []\n",
        "for i in range(results_features_1by1_df.shape[0]):\n",
        "  for j in range(results_features_1by1_df.shape[1]):\n",
        "    if j == 0:\n",
        "      feature = results_features_1by1_df.loc[i][0]\n",
        "      feature_names.append(feature)\n",
        "    if j > 0:\n",
        "      avg_result += results_features_1by1_df.loc[i][j]\n",
        "  avg_result = avg_result/3\n",
        "  print(\"Feature [\"+ feature + \"] overall result: \", avg_result)\n",
        "  \n",
        "  features_avg.append(avg_result)\n",
        "  avg_result = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "LlxusdbrMOmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14b7756-cba5-4d6d-810e-dab6be906545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contribution of features when added to the model:\n",
            "[('Stem', 0.9307), ('+POS_tag', 0.0275), ('+Chunk_tag', 0.0022), ('+is_BOS', -0.0004), ('+is_EOS', 0.0003), ('+is_first_letter_uppercase', 0.0069), ('+word_shape', 0.0022), ('+short_word_shape', -0.0002), ('+has_number', 0.0001), ('+has_hyphen', 0.0), ('+is_upper_has_digit_has_dash', -0.0001), ('+prefix', 0.0026), ('+suffix', 0.0007), ('+is_all_upper', 0.0001), ('+is_stopword', -0.0), ('+word_previous', 0.0031), ('+word_previous_shape', 0.0011), ('+word_previous_short_shape', -0.0), ('+word_after', 0.0012), ('+word_after_shape', 0.001), ('+word_after_short_shape', 0.0002), ('+is_in_gazetteer', -0.0001)]\n"
          ]
        }
      ],
      "source": [
        "# Contribution of each feature after added to the model\n",
        "feature_diffs = []\n",
        "features_avg.insert(0, 0)\n",
        "for i in range(1,len(features_avg)):\n",
        "    x = features_avg[i] - features_avg[i-1]\n",
        "    feature_diffs.append(round(x,4))\n",
        "features_avg.pop(0)\n",
        "features_effect_to_score = list(zip(feature_names, feature_diffs))\n",
        "print(\"Contribution of features when added to the model:\")\n",
        "print(features_effect_to_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "zcv4vBAANDE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd574ab-1317-4558-b5c8-3584cc9cfb06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contribution of features [Sorted in descending order]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Stem', 0.9307),\n",
              " ('+POS_tag', 0.0275),\n",
              " ('+is_first_letter_uppercase', 0.0069),\n",
              " ('+word_previous', 0.0031),\n",
              " ('+prefix', 0.0026),\n",
              " ('+Chunk_tag', 0.0022),\n",
              " ('+word_shape', 0.0022),\n",
              " ('+word_after', 0.0012),\n",
              " ('+word_previous_shape', 0.0011),\n",
              " ('+word_after_shape', 0.001),\n",
              " ('+suffix', 0.0007),\n",
              " ('+is_EOS', 0.0003),\n",
              " ('+word_after_short_shape', 0.0002),\n",
              " ('+has_number', 0.0001),\n",
              " ('+is_all_upper', 0.0001),\n",
              " ('+has_hyphen', 0.0),\n",
              " ('+is_stopword', -0.0),\n",
              " ('+word_previous_short_shape', -0.0),\n",
              " ('+is_upper_has_digit_has_dash', -0.0001),\n",
              " ('+is_in_gazetteer', -0.0001),\n",
              " ('+short_word_shape', -0.0002),\n",
              " ('+is_BOS', -0.0004)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Sort contributions of each features score\n",
        "contribution_of_scores = copy.deepcopy(features_effect_to_score)\n",
        "contribution_of_scores = sorted(contribution_of_scores, key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "print(\"Contribution of features [Sorted in descending order]\")\n",
        "contribution_of_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CLassification Report of the Best Model"
      ],
      "metadata": {
        "id": "eqDufX3f5MuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "sZpzeC66GmO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa5ce19-6ace-4efd-cba0-79d7d975200f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results of the Best Model: \n",
            "--------------------------------------------\n",
            "Precision: 0.9633327236178337\n",
            "Recall:  0.9619683428448369\n",
            "F1 Score:  0.9624381127689882\n"
          ]
        }
      ],
      "source": [
        "# Displaying the classification report for the best model\n",
        "features_remove_for_best_model = ('has_hyphen', 'word_previous_short_shape', 'is_in_gazetteer','is_upper_has_digit_has_dash', 'is_stopword', 'short_word_shape', 'is_BOS')\n",
        "train_sents_for_best_model = copy.deepcopy(train_sents)\n",
        "val_sents_for_best_model = copy.deepcopy(val_sents)\n",
        "test_sents_for_best_model = copy.deepcopy(test_sents)\n",
        "\n",
        "train_sents_for_best_model = drop_dict_keys(train_sents_for_best_model, features_remove_for_best_model)\n",
        "val_sents_for_best_model = drop_dict_keys(val_sents_for_best_model, features_remove_for_best_model)\n",
        "test_sents_for_best_model = drop_dict_keys(test_sents_for_best_model, features_remove_for_best_model)\n",
        "\n",
        "overall_train_sents_model = train_sents_for_best_model + val_sents_for_best_model\n",
        "overall_train_labels_model = train_labels + val_labels\n",
        "\n",
        "\n",
        "\n",
        "crf_best_model_estimator = sklearn_crfsuite.CRF(c1=0.05, c2=0.25, keep_tempfiles=None)\n",
        "crf_best_model_estimator.fit(overall_train_sents_model, overall_train_labels_model)\n",
        "crf_best_model_estimator_preds = crf_best_model_estimator.predict(test_sents_for_best_model)\n",
        "\n",
        "precisions_crf_best_model = sklearn_crfsuite.metrics.flat_precision_score(test_labels, crf_best_model_estimator_preds, average = 'weighted')\n",
        "recalls_crf_best_model = sklearn_crfsuite.metrics.flat_recall_score(test_labels, crf_best_model_estimator_preds, average = 'weighted')\n",
        "f1_scores_crf_best_model = sklearn_crfsuite.metrics.flat_f1_score(test_labels, crf_best_model_estimator_preds, average = 'weighted')\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nResults of the Best Model: \"+ \"\\n\"+\n",
        "      \"--------------------------------------------\")\n",
        "print(\"Precision:\", precisions_crf_best_model)\n",
        "print(\"Recall: \", recalls_crf_best_model)\n",
        "print(\"F1 Score: \", f1_scores_crf_best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4DNBgU6B4Qs"
      },
      "source": [
        "## Recurrent Neural Network (RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKzNgh2LxIYC"
      },
      "source": [
        "###Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "h6b7iz44B6kb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Model, Input, Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, Input, Dropout, LSTM, TimeDistributed, Bidirectional, SimpleRNN\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "cfVSNob9Ub_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9f857e-8cc8-4575-c138-f2c94cc8419d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tytWwE4xN-p"
      },
      "source": [
        "###Prepare the Datasets by DeepCopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GlfibHodsgFQ"
      },
      "outputs": [],
      "source": [
        "# Prepare DeepCopies of train_sents, train_labels, val_sent, val_labels (just in case)\n",
        "train_sents_rnn = copy.deepcopy(train_sents)\n",
        "train_labels_rnn = copy.deepcopy(train_labels)\n",
        "\n",
        "val_sents_rnn = copy.deepcopy(val_sents)\n",
        "val_labels_rnn = copy.deepcopy(val_labels)\n",
        "\n",
        "test_sents_rnn = copy.deepcopy(test_sents)\n",
        "test_labels_rnn = copy.deepcopy(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y80XEf_xUkh"
      },
      "source": [
        "###Find MAX_LENGTH's for Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ipxv6HM9oxz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d14f411-0f64-41c9-fb97-5996c449aba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the Longest Sentences: 124\n",
            "Length of the Longest Label: 124\n",
            "Max Length: 124\n"
          ]
        }
      ],
      "source": [
        "# Find Word and Label with Max Length\n",
        "def FIND_MAX_LENGTH_WORD(data_sents):\n",
        "  MAX_SENT_LENGTH = 0\n",
        "  for sentence in data_sents:\n",
        "    if len(sentence) > MAX_SENT_LENGTH:\n",
        "      MAX_SENT_LENGTH = len(sentence)\n",
        "\n",
        "  return MAX_SENT_LENGTH\n",
        "\n",
        "\n",
        "\n",
        "def FIND_MAX_LENGTH_LABEL(data_labels):\n",
        "  MAX_LENGTH_LABEL = 0\n",
        "  for sentence in data_labels:\n",
        "    if len(sentence) > MAX_LENGTH_LABEL:\n",
        "      MAX_LENGTH_LABEL = len(sentence)\n",
        "\n",
        "  return MAX_LENGTH_LABEL\n",
        "\n",
        "\n",
        "\n",
        "MAX_LENGTH_train_sents = FIND_MAX_LENGTH_WORD(train_sents_rnn)\n",
        "MAX_LENGTH_train_labels = FIND_MAX_LENGTH_LABEL(train_labels_rnn)\n",
        "\n",
        "MAX_LENGTH_val_sents = FIND_MAX_LENGTH_WORD(val_sents_rnn)\n",
        "MAX_LENGTH_val_labels = FIND_MAX_LENGTH_LABEL(val_labels_rnn)\n",
        "\n",
        "MAX_LENGTH_test_sents = FIND_MAX_LENGTH_WORD(test_sents_rnn)\n",
        "MAX_LENGTH_test_labels = FIND_MAX_LENGTH_LABEL(test_labels_rnn)\n",
        "\n",
        "MAX_LENGTH_all_sents = max(MAX_LENGTH_train_sents, MAX_LENGTH_val_sents, MAX_LENGTH_test_sents)\n",
        "MAX_LENGTH_all_labels = max(MAX_LENGTH_train_labels, MAX_LENGTH_val_labels, MAX_LENGTH_test_labels)\n",
        "\n",
        "MAX_LENGTH_overall = max(MAX_LENGTH_all_sents, MAX_LENGTH_all_labels)\n",
        "\n",
        "print(\"Length of the Longest Sentences:\", MAX_LENGTH_all_sents)\n",
        "print(\"Length of the Longest Label:\", MAX_LENGTH_all_labels)\n",
        "print(\"Max Length:\", MAX_LENGTH_overall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8xrMG38xcje"
      },
      "source": [
        "###Find Unique Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "c46yqjBiB6sA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76259ca4-fb9a-4f6b-d15a-5a6036a83cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in the dictionary:  dict_keys(['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC'])\n"
          ]
        }
      ],
      "source": [
        "# find unique labels and create dictionary to map each label to a unique integer value\n",
        "UniqueLabels = {}\n",
        "\n",
        "for sent in train_labels_rnn:\n",
        "  for i in sent: \n",
        "    UniqueLabels[i] = \"\"\n",
        "\n",
        "for sent in val_labels_rnn:\n",
        "  for i in sent: \n",
        "    UniqueLabels[i] = \"\"\n",
        "\n",
        "for sent in test_labels_rnn:\n",
        "  for i in sent: \n",
        "    UniqueLabels[i] = \"\"\n",
        "\n",
        "print(\"Unique labels in the dictionary: \", UniqueLabels.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHqEgdANxoAt"
      },
      "source": [
        "###Map Labels to Integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fcIltfVk1RkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e8cae1-c878-441c-cccd-cadd75a7469e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Labels After Mapping:\n",
            " [[[0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0]]]\n",
            "Validation Labels After Mapping:\n",
            " [[[1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0]]]\n",
            "Test Labels After Mapping:\n",
            " [[1, 1, 6, 1, 1, 1, 1, 4, 1, 1, 1, 1], [4, 5], [6, 1, 6, 9, 9, 1]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Function for mapping values; 5 --> 4, 4 --> 3, 3 --> 2, 2 --> 1, 1 --> 0\n",
        "labels_map = {'O': 1, 'B-ORG': 2, 'B-MISC': 3, 'B-PER': 4, 'I-PER': 5, 'B-LOC': 6, 'I-ORG': 7, 'I-MISC': 8, 'I-LOC': 9}\n",
        "\n",
        "labels_to_list_map = {\n",
        "  1: [1,0,0,0,0,0,0,0,0], 2: [0,1,0,0,0,0,0,0,0], 3: [0,0,1,0,0,0,0,0,0], 4: [0,0,0,1,0,0,0,0,0],\n",
        "  5: [0,0,0,0,1,0,0,0,0], 6: [0,0,0,0,0,1,0,0,0], 7: [0,0,0,0,0,0,1,0,0], 8: [0,0,0,0,0,0,0,1,0],\n",
        "  9: [0,0,0,0,0,0,0,0,1], 0: [0,0,0,0,0,0,0,0,0]\n",
        "}\n",
        "\n",
        "# Turn labels into numeric format\n",
        "# Reference: https://stackoverflow.com/questions/69411748/map-elements-of-a-list-of-lists-to-a-dictionary-value\n",
        "def DEEP_MAP(func, dataset_):\n",
        "    if not isinstance(dataset_, list):\n",
        "        return func(dataset_)\n",
        "    return [DEEP_MAP(func, x) for x in dataset_]\n",
        "\n",
        "train_labels_rnn = DEEP_MAP(lambda x: labels_map[x], train_labels_rnn)\n",
        "val_labels_rnn = DEEP_MAP(lambda x: labels_map[x], val_labels_rnn)\n",
        "test_labels_rnn = DEEP_MAP(lambda x: labels_map[x], test_labels_rnn)\n",
        "\n",
        "train_labels_rnn = DEEP_MAP(lambda x: labels_to_list_map[x], train_labels_rnn)\n",
        "val_labels_rnn = DEEP_MAP(lambda x: labels_to_list_map[x], val_labels_rnn)\n",
        "#test_labels_rnn = DEEP_MAP(lambda x: labels_to_list_map[x], test_labels_rnn)\n",
        "\n",
        "\n",
        "print(\"Train Labels After Mapping:\\n\", train_labels_rnn[:3])\n",
        "print(\"Validation Labels After Mapping:\\n\", val_labels_rnn[:3])\n",
        "print(\"Test Labels After Mapping:\\n\", test_labels_rnn[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-hPASzyxubP"
      },
      "source": [
        "###Turn Dataset into List of List Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "7ARlb489-tQa"
      },
      "outputs": [],
      "source": [
        "# Turn datasets into list of list format instead of list of list of dict\n",
        "# Just apply this on sents, since labels are already in list of lists format \n",
        "def LIST_OF_LIST_SENTS(data_sents):\n",
        "  temp_paragraph = []\n",
        "  temp_sent = []\n",
        "  for sentence in data_sents:\n",
        "    for word in sentence:\n",
        "      stem = word.get('stem')\n",
        "      temp_sent.append(stem)\n",
        "    temp_paragraph.append(temp_sent)\n",
        "    temp_sent = []\n",
        "\n",
        "  return temp_paragraph\n",
        "\n",
        "# train_sents\n",
        "train_sents_rnn_lol = LIST_OF_LIST_SENTS(train_sents_rnn)\n",
        "\n",
        "# val_sents\n",
        "val_sents_rnn_lol = LIST_OF_LIST_SENTS(val_sents_rnn)\n",
        "\n",
        "# test_sents\n",
        "test_sents_rnn_lol = LIST_OF_LIST_SENTS(test_sents_rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evyRCCDex3fg"
      },
      "source": [
        "###Tokenize the Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "01k2usQL7yOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a3b331-c2b9-4dee-a238-80d8ae4d0513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Train Sents: [[1084, 2168, 230, 185, 6, 3570, 252, 5029, 2], [852, 1884], [830, 173]]\n",
            "Tokenized Validation Sents: [[349, 17, 1716, 142, 65, 18, 419, 41, 5, 327, 2], [116, 293], [304, 791, 3690, 2784, 3676, 303, 168, 15, 1564, 12, 78, 31, 1716, 53, 1884, 28, 36, 227, 8, 1736, 152, 5, 49, 95, 6, 142, 65, 18, 1, 410, 4, 1, 665, 244, 2]]\n",
            "Tokenized Test Sents: [[86, 16, 243, 407, 3862, 115, 2, 204, 5, 1465, 612, 3], [20034, 20035], [8102, 2, 134, 831, 4701, 206]]\n"
          ]
        }
      ],
      "source": [
        "# Tokenizing sentences and labels\n",
        "tokenizer_keras_rnn = Tokenizer(filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'')\n",
        "\n",
        "# train_sents\n",
        "tokenizer_keras_rnn.fit_on_texts(train_sents_rnn_lol)\n",
        "train_sents_rnn_tok = tokenizer_keras_rnn.texts_to_sequences(train_sents_rnn_lol)\n",
        "\n",
        "# val_sents\n",
        "tokenizer_keras_rnn.fit_on_texts(val_sents_rnn_lol)\n",
        "val_sents_rnn_tok = tokenizer_keras_rnn.texts_to_sequences(val_sents_rnn_lol)\n",
        "\n",
        "# test_sents\n",
        "tokenizer_keras_rnn.fit_on_texts(test_sents_rnn_lol)\n",
        "test_sents_rnn_tok = tokenizer_keras_rnn.texts_to_sequences(test_sents_rnn_lol)\n",
        "\n",
        "print(\"Tokenized Train Sents:\", train_sents_rnn_tok[:3])\n",
        "print(\"Tokenized Validation Sents:\", val_sents_rnn_tok[:3])\n",
        "print(\"Tokenized Test Sents:\", test_sents_rnn_tok[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOnZ-l8Px7f2"
      },
      "source": [
        "###Padding the Labels and Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Bkw51uf9k6ix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02bd1396-fa0b-47a3-f71e-4b3c4980e913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded Train Sents:\n",
            " [[1084 2168  230  185    6 3570  252 5029    2    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]] \n",
            "\n",
            "Padded Train Labels:\n",
            " [[[0 1 0 ... 0 0 0]\n",
            "  [1 0 0 ... 0 0 0]\n",
            "  [0 0 1 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]] \n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "Padded Validation Sents:\n",
            " [[ 349   17 1716  142   65   18  419   41    5  327    2    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]] \n",
            "\n",
            "Padded Validation Labels:\n",
            " [[[1 0 0 ... 0 0 0]\n",
            "  [1 0 0 ... 0 0 0]\n",
            "  [0 1 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]] \n",
            "\n",
            "-------------------------------------------------------------------------\n",
            "Padded Test Sents:\n",
            " [[  86   16  243  407 3862  115    2  204    5 1465  612    3    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]] \n",
            "\n",
            "Padded Test Labels:\n",
            " [[1 1 6 1 1 1 1 4 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# preprare your dataset for RNN classifier (you need to add padding to labels as well)\n",
        "# Padding sentences and labels\n",
        "\n",
        "# train_sents\n",
        "padded_train_sents_rnn = pad_sequences(train_sents_rnn_tok, padding='post', maxlen=MAX_LENGTH_overall)\n",
        "\n",
        "# train_labels\n",
        "padded_train_labels_rnn = pad_sequences(train_labels_rnn, padding='post', maxlen=MAX_LENGTH_overall)\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# val_sents\n",
        "padded_val_sents_rnn = pad_sequences(val_sents_rnn_tok, padding='post', maxlen=MAX_LENGTH_overall)\n",
        "\n",
        "# val_labels\n",
        "padded_val_labels_rnn = pad_sequences(val_labels_rnn, padding='post', maxlen=MAX_LENGTH_overall)\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# test_sents\n",
        "padded_test_sents_rnn = pad_sequences(test_sents_rnn_tok, padding='post', maxlen=MAX_LENGTH_overall)\n",
        "\n",
        "# test_labels\n",
        "padded_test_labels_rnn = pad_sequences(test_labels_rnn, padding='post', maxlen=MAX_LENGTH_overall)\n",
        "\n",
        "\n",
        "print(\"Padded Train Sents:\\n\", padded_train_sents_rnn[:1], \"\\n\")\n",
        "print(\"Padded Train Labels:\\n\", padded_train_labels_rnn[:1], \"\\n\")\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "print(\"Padded Validation Sents:\\n\", padded_val_sents_rnn[:1], \"\\n\")\n",
        "print(\"Padded Validation Labels:\\n\", padded_val_labels_rnn[:1], \"\\n\")\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "print(\"Padded Test Sents:\\n\", padded_test_sents_rnn[:1], \"\\n\")\n",
        "print(\"Padded Test Labels:\\n\", padded_test_labels_rnn[:1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_train_sents_rnn.shape"
      ],
      "metadata": {
        "id": "XF0MuAgcVLyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "156b85e2-0fd4-4ac8-f205-5af8ab416c05"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14041, 124)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMG61hhMyGot"
      },
      "source": [
        "###Create the Embedding Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "HyOajas3ESnc"
      },
      "outputs": [],
      "source": [
        "# RANDOMLY INITIALIZED\n",
        "random_emb_layer_rnn = Embedding(input_dim=len(tokenizer_keras_rnn.word_index)+1, output_dim=64, input_length=MAX_LENGTH_overall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "IeGJ_Fa6EHQS"
      },
      "outputs": [],
      "source": [
        "# FROM SCRATCH\n",
        "my_rnn_embedding = Word2Vec(sentences = train_sents_rnn_lol, min_count=1)\n",
        "my_rnn_emb_layer = Embedding(input_dim=len(my_rnn_embedding.wv.vocab), output_dim=100, input_length=MAX_LENGTH_overall, weights=[my_rnn_embedding.wv.vectors])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "EVMM7jLV8ch7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21638725-ba01-4f54-b0a2-a834d46437e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Create your own word embeddings from scratch and load a pretrained word embeddings\n",
        "# You can check https://radimrehurek.com/gensim/models/word2vec.html for training a word embeddings from scratch\n",
        "# You can check https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html and https://github.com/RaRe-Technologies/gensim-data for loading pretrained word embeddings.\n",
        "\n",
        "# PRETRAINED\n",
        "pretrained_rnn_embedding = Word2Vec(sentences=api.load('text8'), min_count=1)\n",
        "pretrained_rnn_emb_layer = Embedding(input_dim=len(pretrained_rnn_embedding.wv.vocab), output_dim=100, input_length=MAX_LENGTH_overall, weights=[pretrained_rnn_embedding.wv.vectors])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsGkDzrKyK1i"
      },
      "source": [
        "###Create the RNN Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn labels into numeric format\n",
        "# Reference: https://stackoverflow.com/questions/69411748/map-elements-of-a-list-of-lists-to-a-dictionary-value\n",
        "def DEEP_MAP(func, dataset_):\n",
        "    if not isinstance(dataset_, list):\n",
        "        return func(dataset_)\n",
        "    return [DEEP_MAP(func, x) for x in dataset_]"
      ],
      "metadata": {
        "id": "7nC6jryCSxri"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Kv-rwcFKU0G3"
      },
      "outputs": [],
      "source": [
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "  d = []\n",
        "  for label_list in encoded_seq[0]:\n",
        "    e = []\n",
        "    for vector in label_list:\n",
        "      e.append(np.argmax(vector) + 1)\n",
        "    d.append(e)\n",
        "  return d\n",
        "# define a function to remove paddings and align labels and tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_map(sequence):\n",
        "  # Function for mapping values; 5 --> 4, 4 --> 3, 3 --> 2, 2 --> 1, 1 --> 0\n",
        "  reverse_labels_map = {1: 'O', 2: 'B-ORG', 3: 'B-MISC', 4: 'B-PER', 5: 'I-PER', 6: 'B-LOC', 7: 'I-ORG', 8: 'I-MISC', 9:'I-LOC'}\n",
        "\n",
        "  sequence =  DEEP_MAP(lambda x: reverse_labels_map[x], sequence)\n",
        "  return sequence"
      ],
      "metadata": {
        "id": "uNDb7yL_SUf7"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ZnCTzRyQ_R"
      },
      "source": [
        "####SimpleRNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "NuTzFWwUgwlQ"
      },
      "outputs": [],
      "source": [
        "def RNN_Model_Generator(op_name, embedding_layer_opt, rnn_layer_size, hidden_layer_size, \n",
        "                            padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn):\n",
        "\n",
        "    print(op_name)\n",
        "    print(\"=================================\")\n",
        "    print(\"RNN Hidden Layer Size: \", rnn_layer_size)\n",
        "    print(\"Hidden Layer Size: \", hidden_layer_size)\n",
        "    print(\"--------------------------------------------------------------------- \\n\")\n",
        "\n",
        "\n",
        "    # Define the Model\n",
        "    # --------------------------------------------------------------------------\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add Embedding Layer\n",
        "    model.add(embedding_layer_opt)\n",
        "\n",
        "    # Add RNN\n",
        "    model.add(SimpleRNN(units=rnn_layer_size, return_sequences=True))\n",
        "\n",
        "    model.add(Dropout(rate=0.02))\n",
        "\n",
        "    # Add Dense Layer\n",
        "    model.add(Dense(hidden_layer_size, activation='relu'))\n",
        "\n",
        "    # Add timeDistributed Layer\n",
        "    model.add(TimeDistributed(Dense(9, activation=\"softmax\")))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    print(\"\\n--------------------------------------------------------------------- \\n\")\n",
        "\n",
        "    model.fit(padded_train_sents_rnn, padded_train_labels_rnn, epochs=10, verbose=1)\n",
        "    pred_labels = model.predict(padded_val_sents_rnn)\n",
        "\n",
        "    pred_labels_new = one_hot_decode(pred_labels)\n",
        "    pred_labels_newest = reverse_map(pred_labels_new)\n",
        "\n",
        "    padded_val_labels_rnn_new = one_hot_decode(padded_val_labels_rnn)\n",
        "    padded_val_labels_rnn_newest = reverse_map(padded_val_labels_rnn_new)\n",
        "    print(\"F1 Score: {:.4%}\".format(f1_score(padded_val_labels_rnn_newest, pred_labels_newest)))\n",
        "    print(\"Precision Score: {:.4%}\".format(precision_score(padded_val_labels_rnn_newest, pred_labels_newest)))\n",
        "    print(\"Recall Score: {:.4%}\".format(recall_score(padded_val_labels_rnn_newest, pred_labels_newest)))\n",
        "    print(\"\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1rMygoAyUvk"
      },
      "source": [
        "####LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "d7KD3sCPhZIT"
      },
      "outputs": [],
      "source": [
        "def LSTM_Model_Generator(op_name, embedding_layer_opt, rnn_layer_size, hidden_layer_size, \n",
        "                            padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn):\n",
        "  \n",
        "    print(op_name)\n",
        "    print(\"=================================\")\n",
        "    print(\"RNN Hidden Layer Size: \", rnn_layer_size)\n",
        "    print(\"Hidden Layer Size: \", hidden_layer_size)\n",
        "    print(\"--------------------------------------------------------------------- \\n\")\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add Embedding layer\n",
        "    model.add(embedding_layer_opt)\n",
        "\n",
        "    # Add LSTM\n",
        "    model.add\n",
        "    model.add(LSTM(units=rnn_layer_size, return_sequences=True))\n",
        "\n",
        "    model.add(Dropout(rate=0.02))\n",
        "\n",
        "    model.add(Dense(hidden_layer_size, activation='relu'))\n",
        "\n",
        "    # Add timeDistributed Layer\n",
        "    model.add(TimeDistributed(Dense(9, activation=\"softmax\")))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    #model.summary()\n",
        "    model.fit(padded_train_sents_rnn, padded_train_labels_rnn, epochs=10, verbose=1)\n",
        "    pred_labels = model.predict(padded_val_sents_rnn)\n",
        "\n",
        "    pred_labels_new = one_hot_decode(pred_labels)\n",
        "    pred_labels_newest = reverse_map(pred_labels_new)\n",
        "\n",
        "    padded_val_labels_rnn_new = one_hot_decode(padded_val_labels_rnn)\n",
        "    padded_val_labels_rnn_newest = reverse_map(padded_val_labels_rnn_new)\n",
        "    print(\"F1 Score: {:.4%}\".format(f1_score(padded_val_labels_rnn_newest, pred_labels_newest)))\n",
        "    print(\"Precision Score: {:.4%}\".format(precision_score(padded_val_labels_rnn_newest, pred_labels_newest)))\n",
        "    print(\"Recall Score: {:.4%}\".format(recall_score(padded_val_labels_rnn_newest, pred_labels_newest)))\n",
        "    print(\"\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzN5VbAZymlO"
      },
      "source": [
        "####Bidirectional LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "6DbhQz268cs8"
      },
      "outputs": [],
      "source": [
        "# Create your models and train them\n",
        "# RNN Model Reference: https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54\n",
        "\n",
        "def Bi_LSTM_Model_Generator(op_name, embedding_layer_opt, rnn_layer_size, hidden_layer_size, \n",
        "                            padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn):\n",
        "  \n",
        "    print(op_name)\n",
        "    print(\"=================================\")\n",
        "    print(\"RNN Hidden Layer Size: \", rnn_layer_size)\n",
        "    print(\"Hidden Layer Size: \", hidden_layer_size)\n",
        "    print(\"--------------------------------------------------------------------- \\n\")\n",
        "\n",
        "    model = Sequential()\n",
        "    # Add Embedding layer\n",
        "    model.add(embedding_layer_opt)\n",
        "\n",
        "    # Add bidirectional LSTM\n",
        "    model.add(Bidirectional(LSTM(units=rnn_layer_size, return_sequences=True)))\n",
        "\n",
        "    model.add(Dropout(rate=0.02))\n",
        "\n",
        "    model.add(Dense(hidden_layer_size, activation='relu'))\n",
        "\n",
        "    # Add timeDistributed Layer\n",
        "    model.add(TimeDistributed(Dense(9, activation=\"softmax\")))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "  \n",
        "    model.fit(padded_train_sents_rnn, padded_train_labels_rnn, epochs=10, verbose=1)\n",
        "    pred_labels = model.predict(padded_val_sents_rnn)\n",
        "\n",
        "    pred_labels_new = one_hot_decode(pred_labels)\n",
        "    pred_labels_newest = reverse_map(pred_labels_new)\n",
        "\n",
        "    padded_val_labels_rnn_new = one_hot_decode(padded_val_labels_rnn)\n",
        "    padded_val_labels_rnn_newest = reverse_map(padded_val_labels_rnn_new)\n",
        "    print(\"F1 Score: {:.4%}\".format(f1_score(padded_val_labels_rnn_newest, pred_labels_newest)))\n",
        "    print(\"Precision Score: {:.4%}\".format(precision_score(padded_val_labels_rnn_newest, pred_labels_newest)))\n",
        "    print(\"Recall Score: {:.4%}\".format(recall_score(padded_val_labels_rnn_newest, pred_labels_newest)))\n",
        "    print(\"\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u6N0KSmEXFk1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0KyD726ysxk"
      },
      "source": [
        "###Generate the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "GvliAwSrnZ_3"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "qHB38Kt2U6N1"
      },
      "outputs": [],
      "source": [
        "# General\n",
        "operation_names = ['Random Embedded Model', 'Embedded From Scratch Model', 'Pretrained Embedding Model']\n",
        "embedding_layer_options = [random_emb_layer_rnn, my_rnn_emb_layer, pretrained_rnn_emb_layer]\n",
        "RNN_layer_size_options = [32, 64]\n",
        "hidden_layer_options =[16, 24]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "019f5TuRoKgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce1a4e8-d78c-4051-dc3f-1c6b3efc052a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  32\n",
            "Hidden Layer Size:  16\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 124, 64)           1409024   \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 124, 32)           3104      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 124, 32)           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 124, 16)           528       \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 124, 9)           153       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,412,809\n",
            "Trainable params: 1,412,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 51s 115ms/step - loss: 0.0453 - accuracy: 0.8687\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0101 - accuracy: 0.9793\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 51s 115ms/step - loss: 0.0060 - accuracy: 0.9910\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0045 - accuracy: 0.9791\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 48s 110ms/step - loss: 0.0037 - accuracy: 0.9489\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0031 - accuracy: 0.8365\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0027 - accuracy: 0.9205\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 49s 111ms/step - loss: 0.0023 - accuracy: 0.9077\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 50s 114ms/step - loss: 0.0021 - accuracy: 0.8510\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 49s 111ms/step - loss: 0.0019 - accuracy: 0.8574\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  32\n",
            "Hidden Layer Size:  16\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 5s 9ms/step - loss: 0.0604 - accuracy: 0.5382\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0128 - accuracy: 0.6453\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0066 - accuracy: 0.9897\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0047 - accuracy: 0.9912\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0039 - accuracy: 0.9914\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0033 - accuracy: 0.9919\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0029 - accuracy: 0.9923\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0027 - accuracy: 0.9922\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0023 - accuracy: 0.9922\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0022 - accuracy: 0.9923\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  32\n",
            "Hidden Layer Size:  16\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 124, 64)           1409024   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 124, 64)          24832     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 124, 64)           0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 124, 16)           1040      \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 124, 9)           153       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,435,049\n",
            "Trainable params: 1,435,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 8s 13ms/step - loss: 0.0471 - accuracy: 0.9198\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 0.0089 - accuracy: 0.9912\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 0.0041 - accuracy: 0.9589\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 0.0024 - accuracy: 0.9341\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 0.0017 - accuracy: 0.9836\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 0.0012 - accuracy: 0.9787\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 8.7398e-04 - accuracy: 0.9778\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 6.4876e-04 - accuracy: 0.9468\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 4.9653e-04 - accuracy: 0.8321\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 4.4557e-04 - accuracy: 0.7391\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  32\n",
            "Hidden Layer Size:  24\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 124, 64)           1409024   \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 124, 32)           3104      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 124, 32)           0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 124, 24)           792       \n",
            "                                                                 \n",
            " time_distributed_4 (TimeDis  (None, 124, 9)           225       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,413,145\n",
            "Trainable params: 1,413,145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 49s 110ms/step - loss: 0.0386 - accuracy: 0.9622\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 50s 114ms/step - loss: 0.0071 - accuracy: 0.9960\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 49s 111ms/step - loss: 0.0044 - accuracy: 0.9969\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0034 - accuracy: 0.9966\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0028 - accuracy: 0.9965\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 49s 110ms/step - loss: 0.0025 - accuracy: 0.9971\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0022 - accuracy: 0.9971\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 51s 115ms/step - loss: 0.0020 - accuracy: 0.9965\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 49s 111ms/step - loss: 0.0019 - accuracy: 0.9969\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 49s 111ms/step - loss: 0.0018 - accuracy: 0.9962\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  32\n",
            "Hidden Layer Size:  24\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 6s 9ms/step - loss: 0.0486 - accuracy: 0.9792\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0090 - accuracy: 0.9911\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0043 - accuracy: 0.9928\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0031 - accuracy: 0.9954\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0025 - accuracy: 0.9956\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0022 - accuracy: 0.9964\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0020 - accuracy: 0.9944\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 4s 8ms/step - loss: 0.0018 - accuracy: 0.9967\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0016 - accuracy: 0.9953\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0015 - accuracy: 0.9974\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  32\n",
            "Hidden Layer Size:  24\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 124, 64)           1409024   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 124, 64)          24832     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 124, 64)           0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 124, 24)           1560      \n",
            "                                                                 \n",
            " time_distributed_6 (TimeDis  (None, 124, 9)           225       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,435,641\n",
            "Trainable params: 1,435,641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 9s 13ms/step - loss: 0.0373 - accuracy: 0.9828\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 0.0050 - accuracy: 0.9949\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 0.0022 - accuracy: 0.9976\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 0.0013 - accuracy: 0.9984\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 8.9343e-04 - accuracy: 0.9970\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 6.0855e-04 - accuracy: 0.9987\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 4.5554e-04 - accuracy: 0.9982\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 3.5349e-04 - accuracy: 0.9989\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 3.0428e-04 - accuracy: 0.9969\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 6s 13ms/step - loss: 3.0609e-04 - accuracy: 0.9911\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  64\n",
            "Hidden Layer Size:  16\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 124, 64)           1409024   \n",
            "                                                                 \n",
            " simple_rnn_2 (SimpleRNN)    (None, 124, 64)           8256      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 124, 64)           0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 124, 16)           1040      \n",
            "                                                                 \n",
            " time_distributed_7 (TimeDis  (None, 124, 9)           153       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,418,473\n",
            "Trainable params: 1,418,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 51s 114ms/step - loss: 0.0334 - accuracy: 0.7437\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 52s 118ms/step - loss: 0.0053 - accuracy: 0.2151\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 50s 114ms/step - loss: 0.0033 - accuracy: 0.2206\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 50s 113ms/step - loss: 0.0027 - accuracy: 0.2180\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 50s 114ms/step - loss: 0.0022 - accuracy: 0.2196\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 50s 113ms/step - loss: 0.0020 - accuracy: 0.2707\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 51s 115ms/step - loss: 0.0019 - accuracy: 0.2418\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 50s 113ms/step - loss: 0.0019 - accuracy: 0.2477\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 50s 114ms/step - loss: 0.0016 - accuracy: 0.2112\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0015 - accuracy: 0.2046\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  64\n",
            "Hidden Layer Size:  16\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 6s 9ms/step - loss: 0.0541 - accuracy: 0.5415\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0057 - accuracy: 0.1941\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0032 - accuracy: 0.1825\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0024 - accuracy: 0.1736\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0021 - accuracy: 0.1748\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0019 - accuracy: 0.1914\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0016 - accuracy: 0.2081\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0016 - accuracy: 0.2072\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0015 - accuracy: 0.1869\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0013 - accuracy: 0.2326\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  64\n",
            "Hidden Layer Size:  16\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 124, 64)           1409024   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 124, 128)         66048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 124, 128)          0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 124, 16)           2064      \n",
            "                                                                 \n",
            " time_distributed_9 (TimeDis  (None, 124, 9)           153       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,477,289\n",
            "Trainable params: 1,477,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 9s 14ms/step - loss: 0.0271 - accuracy: 0.9842\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 0.0026 - accuracy: 0.8863\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 0.0011 - accuracy: 0.9462\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 6.3770e-04 - accuracy: 0.9059\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 4.6467e-04 - accuracy: 0.8751\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 3.5679e-04 - accuracy: 0.9035\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 2.9610e-04 - accuracy: 0.9895\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 2.5754e-04 - accuracy: 0.8186\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 2.5178e-04 - accuracy: 0.9040\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 3.2863e-04 - accuracy: 0.9426\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  64\n",
            "Hidden Layer Size:  24\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 124, 64)           1409024   \n",
            "                                                                 \n",
            " simple_rnn_3 (SimpleRNN)    (None, 124, 64)           8256      \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 124, 64)           0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 124, 24)           1560      \n",
            "                                                                 \n",
            " time_distributed_10 (TimeDi  (None, 124, 9)           225       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,419,065\n",
            "Trainable params: 1,419,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 52s 116ms/step - loss: 0.0257 - accuracy: 0.9599\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0043 - accuracy: 0.9353\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 50s 113ms/step - loss: 0.0028 - accuracy: 0.9047\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 52s 118ms/step - loss: 0.0023 - accuracy: 0.8527\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0019 - accuracy: 0.8095\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 52s 118ms/step - loss: 0.0017 - accuracy: 0.7931\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0016 - accuracy: 0.8540\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0016 - accuracy: 0.8091\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 49s 112ms/step - loss: 0.0015 - accuracy: 0.9063\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 49s 111ms/step - loss: 0.0015 - accuracy: 0.9306\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  64\n",
            "Hidden Layer Size:  24\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 7s 10ms/step - loss: 0.0338 - accuracy: 0.9862\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0049 - accuracy: 0.7698\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0027 - accuracy: 0.5350\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0021 - accuracy: 0.2301\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 4s 10ms/step - loss: 0.0018 - accuracy: 0.2661\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0016 - accuracy: 0.2238\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0016 - accuracy: 0.3198\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0014 - accuracy: 0.3915\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 4s 10ms/step - loss: 0.0013 - accuracy: 0.3191\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 4s 9ms/step - loss: 0.0012 - accuracy: 0.2445\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Random Embedded Model\n",
            "=================================\n",
            "RNN Hidden Layer Size:  64\n",
            "Hidden Layer Size:  24\n",
            "--------------------------------------------------------------------- \n",
            "\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 124, 64)           1409024   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 124, 128)         66048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 124, 128)          0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 124, 24)           3096      \n",
            "                                                                 \n",
            " time_distributed_12 (TimeDi  (None, 124, 9)           225       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,478,393\n",
            "Trainable params: 1,478,393\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 9s 14ms/step - loss: 0.0254 - accuracy: 0.8230\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 7s 16ms/step - loss: 0.0020 - accuracy: 0.3877\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 8.3124e-04 - accuracy: 0.5209\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 4.5380e-04 - accuracy: 0.3459\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 2.9583e-04 - accuracy: 0.3392\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 2.5456e-04 - accuracy: 0.6695\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 2.5489e-04 - accuracy: 0.8065\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 2.2044e-04 - accuracy: 0.9045\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 2.3369e-04 - accuracy: 0.7538\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 6s 14ms/step - loss: 1.7344e-04 - accuracy: 0.9458\n",
            "F1 Score: 0.0000%\n",
            "Precision Score: 0.0000%\n",
            "Recall Score: 0.0000%\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Random Embedded Model\n",
        "for a in range(len(RNN_layer_size_options)):\n",
        "  for b in range(len(hidden_layer_options)):\n",
        "    RNN_Model_Generator('Random Embedded Model', random_emb_layer_rnn, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn)\n",
        "    LSTM_Model_Generator('Random Embedded Model', random_emb_layer_rnn, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn)\n",
        "    Bi_LSTM_Model_Generator('Random Embedded Model', random_emb_layer_rnn, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx1nVDyxouei"
      },
      "outputs": [],
      "source": [
        "# Embedded From Scratch Model\n",
        "for a in range(len(RNN_layer_size_options)):\n",
        "  for b in range(len(hidden_layer_options)):\n",
        "    RNN_Model_Generator('Embedded From Scratch Model', my_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn)\n",
        "    LSTM_Model_Generator('Embedded From Scratch Model', my_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn)\n",
        "    Bi_LSTM_Model_Generator('Embedded From Scratch Model', my_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDeloKOfo-zO"
      },
      "outputs": [],
      "source": [
        "# Pretrained Embedding Model\n",
        "for a in range(len(RNN_layer_size_options)):\n",
        "  for b in range(len(hidden_layer_options)):\n",
        "    RNN_Model_Generator('Pretrained Embedding Model', pretrained_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn)\n",
        "    LSTM_Model_Generator('Pretrained Embedding Model', pretrained_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn)\n",
        "    Bi_LSTM_Model_Generator('Pretrained Embedding Model', pretrained_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, \n",
        "                            padded_val_sents_rnn, padded_val_labels_rnn, \n",
        "                            padded_test_sents_rnn, padded_test_labels_rnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBt1UKi2NCdA"
      },
      "outputs": [],
      "source": [
        "padded_overall_sents = np.concatenate((padded_train_sents_rnn, padded_val_sents_rnn), axis=0)\n",
        "padded_overall_labels = np.concatenate((padded_train_labels_rnn, padded_val_labels_rnn), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzEkCwJ58cvh"
      },
      "outputs": [],
      "source": [
        "# Evaluate your models with functions of seqeval library\n",
        "# Since every model generates the same results, it is useless to make evaluation on test data using (train + validation) data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDnYCXXEUG_g"
      },
      "source": [
        "## My Report\n",
        "\n",
        "Named Entity Recognition task carried out best with CRF model with very high F1, Precision, and Recall Score (96-97%). Feature extraction step and eliminating the features which has no contribution or the features which decreases the score of the model, improves the overall score of the model in the test data. \n",
        "\n",
        "In the RNN part of the project, due to an \"overfitting\" situation experienced with model; the F1, Precision, and Recall Scores are very very low. I think this situation is caused by the zero's in the data due to padding. Due to the majority of zero's in the data, RNN models assummed that, marking every word with \"zero\" which corresponds to the \"Other\" label provide the best predictions. This conclusion is derived from the epochs. In the epochs, accuracy of the model's are displayed around 93 - 96%. But, when the time for testing the model on the validation set comes, the model always predict the label as zero.\n",
        "\n",
        "An analogy about this situation can be formed as the following:\n",
        "If we provide 90 cat images and 10 dog images to a model and expect model to predict the images we give as the test set, the model will predict every image as \"cat\" in order to work with 90% accuracy.\n",
        "\n",
        "As the result of these experiments it can ben concluded that, CRF models work best for NER task and RNN - LSTM models are not useful for this task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NOTE ABOUT SUBMISSION: \n",
        "I submitted this homework 2 days late because in the first day I was trying to fix some errors and in the 2nd day even if I start running the code in the afternoon, at the middle of the execution somehow my dorm internet get disconnected. Because of that I re-run the code but the code couldn't finish the execution unyil the midnight. Today, I couldn't run the models with \"Embedded From Scratch Model\" and \"Pretrained Embedding Model\" because runtime is disconnected and my GPU limit is finished when I was having my CS412 Machine Learning Final. "
      ],
      "metadata": {
        "id": "MfjH8ai1g6ui"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ECYsXDBl-7mx",
        "M6WgW13i842Y",
        "FuznehOk4yIm",
        "mMt9aGRU5Db9",
        "eqDufX3f5MuJ"
      ],
      "name": "26431.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}