# -*- coding: utf-8 -*-
"""26431.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fcbvd1Rj6jomuY7QA8TzWFPYwqSqNID3

##Initialize Project
"""

!pip install -U 'scikit-learn<0.24'
!pip install sklearn-crfsuite
!pip install tqdm
# YOU NEED TO RESTART THE RUNTIME!!!

# Run this cell to mount your drive to this notebook in order to read the datasets
from google.colab import drive
drive.mount('/content/drive')

import os
import re
import json
import pandas as pd
import numpy as np

import warnings
warnings.filterwarnings("ignore")

"""## Read Dataset"""

# Put the folder path where the datasets are located
PATH = "drive/My Drive/Colab Notebooks/CS445_NLP/Project #2/Datasets/"

def read_data(data):
  temp = []
  with open(data) as f:
        text = f.read()
  paragraph_list = text.split('\n\n')
  for sent in paragraph_list:
    if sent != "-DOCSTART- -X- -X- O":
      if sent != "":
        items = sent.split('\n')
        result = tuple(tuple(i.split(' ')) for i in items)
        temp.append(result)
        #print(tuple(tuple(i.split(' ')) for i in items))
  return temp

final = read_data(PATH+"train.txt")
final[:3]
#DOSSTARTLARI at

# read data with your custom function
train_data = read_data(PATH+"train.txt")
val_data = read_data(PATH+"valid.txt")
test_data = read_data(PATH+"test.txt")

print("Length of the train data: ", len(train_data))
print("Length of the validation data: ", len(val_data))
print("Length of the test data: ", len(test_data))

# Train data
print(train_data[:3])

# Validation data
print(val_data[:5])

# Test Data
print(test_data[:5])

"""# Create Gazetteer"""

def href_regex_extractor(text):
  regex = re.compile("href=\\\"(?:%\d+)?((?:([A-ZIÜŞÖÇ][A-Za-zığüşöç]*))(?:%\d+)?)*\\\"&gt;(\s?[A-ZIÜŞÖÇ][A-Za-zığüşöç]*\s?)*&lt;")
  temp = []
  for m in re.finditer(regex, text): 
    temp.append(m.group())
  return temp

# Load wikipedia pages and extract the href expressions to a list.
from tqdm import tqdm

dir = "/content/drive/My Drive/Colab Notebooks/CS445_NLP/Project #2/Wikipedia_Pages/wikipedia_pages/wikipedia_pages"

hrefs = []
count = 0
for file in tqdm(os.listdir(dir)):
  with open(dir+ '/' + file, 'r') as file:
    if count == 999:
      print("\n\nThis file cannot be processed:")
      print(file)
      count+=1
    else:
      data = json.load(file)
      result = href_regex_extractor(data.get('text'))
      result = ",".join(result)
      hrefs.append(result)
      count+=1

print(len(hrefs))
hrefs[:10]

# REGEX Extract Title
def regex_extract_title(text):
  regex = re.compile("\"((?:%\d+)?[A-ZIÜŞÖÇ0-9][A-Za-zığüşöç0-9]*)*")
  temp = []
  for m in re.finditer(regex, text):

    regex_1 = re.compile("\"")
    subst_1 = ""
    result_1 = re.sub(regex_1, subst_1, m.group(0), 0)

    regex_2 = re.compile("(%\d+)")
    subst_2 = " "
    result_2 = re.sub(regex_2, subst_2, result_1, 0)
    temp.append(result_2)

  return temp
  


count_row = 0
extracted_titles = []

for row in tqdm(hrefs):
  #print(i)
  result_regex = regex_extract_title(row)
  for i in result_regex:
    if i  != '':
      extracted_titles.append(i)

#REGEX Extract Anchor
def regex_extract_anchor(text):
  regex = re.compile("gt;([A-ZIÜŞÖÇ0-9][A-Za-zığüşöç0-9]*( )?)*")
  temp = []
  for m in re.finditer(regex, text):
    regex_2 = re.compile("gt;")
    subst_2 = ""
    result = re.sub(regex_2, subst_2, m.group(0), 0)
    temp.append(result)
  return temp


count_row_anchor = 0
extracted_anchors = []

for row in tqdm(hrefs):
  result_regex_anchor = regex_extract_anchor(row)
  for i in result_regex_anchor:
    extracted_anchors.append(i)

# Extraction Results from JSON Files
# Titles
print("Amount of the extracted titles: ", len(extracted_titles))
print("Sample Extracted Titles:\n")
print(extracted_titles[10:18])

#-------------------------------------------------------------------------------
print("\n\n")
# Anchors
print("Amount of the extracted anchors: ", len(extracted_anchors))
print("Sample ExtractedAnchors:\n")
print(extracted_anchors[10:18])

# Create Gazetteer Word List with all unique words
MyGazetteer = []

# Append titles and anchors to the Gazetteer
MyGazetteer = extracted_titles + extracted_anchors
print("Size of the Gazetteer after adding titles and anchors: ", len(MyGazetteer))

# Remove the dublicates from MyGazetteer
UniqueGazetteer = {}

for i in MyGazetteer:
  UniqueGazetteer[i] = ""

print("Unique Words in the Gazetteer: ", len(UniqueGazetteer.keys()))
print("Gazetteer:")
MyGazetteer[:20]

"""# Models

## Conditional Random Fields (CRF)

### Extract features for CRF
"""

import nltk
from nltk.stem import PorterStemmer

nltk.download('stopwords')
from nltk.corpus import stopwords

import sklearn_crfsuite
from sklearn.metrics import make_scorer
from sklearn_crfsuite import scorers
from sklearn_crfsuite import metrics
from sklearn.model_selection import GridSearchCV

def wordShape(text):
    t1 = re.sub('[A-Z]', 'X',text)
    t2 = re.sub('[a-z]', 'x', t1)
    return re.sub('[0-9]', 'd', t2)

def shortWordShape(text):
    t1 = re.sub('([A-Z]+)', 'X',text)
    t2 = re.sub('([a-z]+)', 'x', t1)
    return re.sub('([0-9]+)', 'd', t2)

def has_number(text):
    return bool(re.search(r'\d', text))

def has_hyphen(text):
    return bool(re.search(r'-', text))

def is_upper_has_digit_has_dash(text):
  result = False
  if text.isupper():
    if has_number(text):
      if bool(re.search(r'-', text)):
        result = True

  return result

def is_stopword(text):
  result = False
  stop_words = set(stopwords.words("english"))
  for word in stop_words:
    if text == word:
      result = True
    else:
      result = False
      
  return result

def is_in_gazetteer(text):
  for word in MyGazetteer:
    result = False
    if text == word:
      return True
    else:
      result = False
      
  return result

from nltk.corpus.reader.wordnet import WordNetError
# create a function to extract features for each token

def token2features(sentence: list, idx: int) -> dict:
  ps = PorterStemmer()
  word = sentence[idx][0]
  features = {
      'stem': "None",
      'POS_tag': "None",
      'Chunk_tag': "None",
      'is_BOS': "None",
      'is_EOS': "None",
      'is_first_letter_uppercase': "None",
      'word_shape': "None",
      'short_word_shape': "None",
      'has_number': "None",
      'has_hyphen': "None",
      'is_upper_has_digit_has_dash': "None",
      'prefix':"None",
      'suffix': "None",
      'is_all_upper': "None",
      'is_stopword': "None",

      # Neighbor Words
      # Word Before
      'word_previous': "None",
      'word_previous_shape': "None",
      'word_previous_short_shape': "None",

      # Word After
      'word_after': "None",
      'word_after_shape': "None",
      'word_after_short_shape': "None",

      'is_in_gazetteer':"None"
  }

  features.update({'stem' : ps.stem(word)}),
  features.update({'POS_tag' : sentence[idx][1]})
  features.update({'Chunk_tag' : sentence[idx][2]})

  if(len(sentence) <= 2):
    features.update({
        'is_BOS' : True,
        'is_EOS' : True
    })
  

  else:
    # FIRST WORD
    if idx == 0:
      features.update({
          'is_BOS' : True,
          'is_EOS' : False,
          'word_after' : sentence[idx+1][0],
          'word_after_shape' : wordShape(sentence[idx+1][0]),
          'word_after_short_shape' : shortWordShape(sentence[idx+1][0]) 
      })
        

    # LAST WORD
    elif idx == len(sentence)-1:
      features.update({
          'is_BOS' : False,
          'is_EOS' : True,
          'word_previous' : sentence[idx-1][0],
          'word_previous_shape' : wordShape(sentence[idx-1][0]),
          'word_previous_short_shape' : shortWordShape(sentence[idx-1][0])
      })


    else:
      #  WORD IN THE MIDDLE
      features.update({
          'is_BOS' : False,
          'is_EOS' : False,
          'word_previous' : sentence[idx-1][0],
          'word_after' : sentence[idx+1][0],

          'word_previous_shape' : wordShape(sentence[idx-1][0]),
          'word_previous_short_shape' : shortWordShape(sentence[idx-1][0]),

          'word_after_shape' : wordShape(sentence[idx+1][0]),
          'word_after_short_shape' : shortWordShape(sentence[idx+1][0])
          
      })
      
  features.update({
      'is_first_letter_uppercase' : word[0].isupper(),
      'word_shape' : wordShape(word),
      'short_word_shape' : shortWordShape(word),
      'has_number' : has_number(word),
      'has_hyphen' : has_hyphen(word),
      'is_upper_has_digit_has_dash' : is_upper_has_digit_has_dash(word),
      'prefix' : word[:4],
      'suffix' : word[-4:],
      'is_all_upper' : word.isupper(),
      'is_stopword' : is_stopword(word),
      'is_in_gazetteer' : is_in_gazetteer(word)
  })
  return features

# define function to process each token given a sentence
def sent2features(sentence: list) -> list:
  return [token2features(sentence, i) for i in range(len(sentence))]

# get named entity labels from the sentence
def sent2labels(sentence: list) -> list:
  NER_labels = []
  for word in sentence:
    NER_labels.append(word[3])
  return NER_labels

# prepare inputs and labels

train_sents = [sent2features(s) for s in tqdm(train_data)]
val_sents = [sent2features(s) for s in tqdm(val_data)]
test_sents = [sent2features(s) for s in tqdm(test_data)]

train_labels = [sent2labels(s) for s in tqdm(train_data)]
val_labels = [sent2labels(s) for s in tqdm(val_data)]
test_labels = [sent2labels(s) for s in tqdm(test_data)]

train_sents[1] # Peter Blackburn

train_labels[1] # Peter Blackburn

"""###Initialize the CRF Model and GridSearchCV"""

# Initialize the CRF model
crf = sklearn_crfsuite.CRF(
    algorithm='lbfgs',
    c1=0.1,
    c2=0.1,
    max_iterations=100,
    all_possible_transitions=True
)

# Set the hyperparameter space that will be scanned.
# TAKES SO MUCH TIME, THEREFORE I AM COMMENTING THIS PART
#crf= sklearn_crfsuite.CRF()

#grid_params_CRF = {
#  "c1": [0, 0.05, 0.25],
#  "c2": [0, 0.05, 0.25]
#}

# initialize GridSearchCV for CRF
#grid_search_CRF = GridSearchCV(
#                 estimator = crf,
#                 param_grid = grid_params_CRF,
#                 verbose = 2)


# fitting the model for grid search 
#grid_search_CRF.fit(train_sents, train_labels)


#print best parameter after tuning 
#print("Best Parameters: ", grid_search_CRF.best_params_)
  
# print how our model looks after hyper-parameter tuning 
#print("Model after hyper-parameter tuning: ", grid_search_CRF.cv_results_['params'][grid_search_CRF.best_index_])
#print("Best Estimator: ", grid_search_CRF.best_estimator_)

# initialize and train a crf model with best hyper-parameters
crf_best_estimator = sklearn_crfsuite.CRF(c1=0.05, c2=0.25, keep_tempfiles=None)

# fit your model
crf_best_estimator.fit(train_sents, train_labels)

# make predictions
preds_crf = crf_best_estimator.predict(val_sents)

# calculate f1-score and classification report for test using sklearn_crfsuite.metrics class
# evaluate on validation set
precision_crf = sklearn_crfsuite.metrics.flat_precision_score(val_labels, preds_crf, average = 'weighted')
recall_crf = sklearn_crfsuite.metrics.flat_recall_score(val_labels, preds_crf, average = 'weighted')
f1_crf = sklearn_crfsuite.metrics.flat_f1_score(val_labels, preds_crf, average = 'weighted')

print("Precision Score:", precision_crf, "\n",
      "Recall Score:", recall_crf, "\n",
      "F1 Score:", f1_crf)

def drop_dict_keys(sent_list, remove_features_list):
  for k in remove_features_list:
    for sentence in sent_list:
      for word in sentence:
        word.pop(k, None)
  return sent_list

def get_crf_results(train_sents_p, val_sents_p, train_labels_, val_labels_):
  crf_best_estimator = sklearn_crfsuite.CRF(c1=0.05, c2=0.25, keep_tempfiles=None)
  crf_best_estimator.fit(train_sents_p, train_labels_)
  crf_best_estimator_preds = crf_best_estimator.predict(val_sents_p)

  precisions_crf = sklearn_crfsuite.metrics.flat_precision_score(val_labels_, crf_best_estimator_preds, average = 'weighted')
  recalls_crf = sklearn_crfsuite.metrics.flat_recall_score(val_labels_, crf_best_estimator_preds, average = 'weighted')
  f1_scores_crf = sklearn_crfsuite.metrics.flat_f1_score(val_labels_, crf_best_estimator_preds, average = 'weighted')

  #results_add_feature_1by1_df = pd.DataFrame({"Features": features_for_add_1by1, "Precision": precisions_1by1, "Recall": recalls_1by1, "F1 Score": f1_scores_1by1})
  result = [precisions_crf, recalls_crf, f1_scores_crf]
  print(result)
  return result

"""###Give Features One By One to the CRF Model"""

#'stem'
import copy
features_remove_1 = ( 
            'POS_tag', 'Chunk_tag', 'is_BOS', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',
            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_1 = copy.deepcopy(train_sents)
val_sents_1 = copy.deepcopy(val_sents)
train_sents_1 = drop_dict_keys(train_sents_1, features_remove_1)
val_sents_1 = drop_dict_keys(val_sents_1, features_remove_1)
result_1 = get_crf_results(train_sents_1, val_sents_1, train_labels, val_labels)





#'stem', 'POS_tag'
features_remove_2 = ('Chunk_tag', 'is_BOS', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',
            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_2 = copy.deepcopy(train_sents)
val_sents_2 = copy.deepcopy(val_sents)
train_sents_2 = drop_dict_keys(train_sents_2, features_remove_2)
val_sents_2 = drop_dict_keys(val_sents_2, features_remove_2)
result_2 = get_crf_results(train_sents_2, val_sents_2, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 
features_remove_3 = ( 'is_BOS', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',
            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_3 = copy.deepcopy(train_sents)
val_sents_3 = copy.deepcopy(val_sents)
train_sents_3 = drop_dict_keys(train_sents_3, features_remove_3)
val_sents_3 = drop_dict_keys(val_sents_3, features_remove_3)
result_3 = get_crf_results(train_sents_3, val_sents_3, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_BOS',
features_remove_4 = ( 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',
            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_4 = copy.deepcopy(train_sents)
val_sents_4 = copy.deepcopy(val_sents)
train_sents_4 = drop_dict_keys(train_sents_4, features_remove_4)
val_sents_4 = drop_dict_keys(val_sents_4, features_remove_4)
result_4 = get_crf_results(train_sents_4, val_sents_4, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS',
features_remove_5 = ( 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',
            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_5 = copy.deepcopy(train_sents)
val_sents_5 = copy.deepcopy(val_sents)
train_sents_5 = drop_dict_keys(train_sents_5, features_remove_5)
val_sents_5 = drop_dict_keys(val_sents_5, features_remove_5)
result_5 = get_crf_results(train_sents_5, val_sents_5, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase',
features_remove_6 = ('word_shape', 'short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',
            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_6 = copy.deepcopy(train_sents)
val_sents_6 = copy.deepcopy(val_sents)
train_sents_6 = drop_dict_keys(train_sents_6, features_remove_6)
val_sents_6 = drop_dict_keys(val_sents_6, features_remove_6)
result_6 = get_crf_results(train_sents_6, val_sents_6, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 
features_remove_7 = ('short_word_shape','has_number','has_hyphen','is_upper_has_digit_has_dash',
            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_7 = copy.deepcopy(train_sents)
val_sents_7 = copy.deepcopy(val_sents)
train_sents_7 = drop_dict_keys(train_sents_7, features_remove_7)
val_sents_7 = drop_dict_keys(val_sents_7, features_remove_7)
result_7 = get_crf_results(train_sents_7, val_sents_7, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape',
features_remove_8 = ('has_number','has_hyphen','is_upper_has_digit_has_dash',
            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_8 = copy.deepcopy(train_sents)
val_sents_8 = copy.deepcopy(val_sents)
train_sents_8 = drop_dict_keys(train_sents_8, features_remove_8)
val_sents_8 = drop_dict_keys(val_sents_8, features_remove_8)
result_8 = get_crf_results(train_sents_8, val_sents_8, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number',
features_remove_9 = ('has_hyphen','is_upper_has_digit_has_dash',
            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_9 = copy.deepcopy(train_sents)
val_sents_9 = copy.deepcopy(val_sents)
train_sents_9 = drop_dict_keys(train_sents_9, features_remove_9)
val_sents_9 = drop_dict_keys(val_sents_9, features_remove_9)
result_9 = get_crf_results(train_sents_9, val_sents_9, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen',
features_remove_10 = ('is_upper_has_digit_has_dash',
            'prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_10 = copy.deepcopy(train_sents)
val_sents_10 = copy.deepcopy(val_sents)
train_sents_10 = drop_dict_keys(train_sents_10, features_remove_10)
val_sents_10 = drop_dict_keys(val_sents_10, features_remove_10)
result_10 = get_crf_results(train_sents_10, val_sents_10, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash',
features_remove_11 = ('prefix', 'suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_11 = copy.deepcopy(train_sents)
val_sents_11 = copy.deepcopy(val_sents)
train_sents_11 = drop_dict_keys(train_sents_11, features_remove_11)
val_sents_11 = drop_dict_keys(val_sents_11, features_remove_11)
result_11 = get_crf_results(train_sents_11, val_sents_11, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 
features_remove_12 = ('suffix','is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_12 = copy.deepcopy(train_sents)
val_sents_12 = copy.deepcopy(val_sents)
train_sents_12 = drop_dict_keys(train_sents_12, features_remove_12)
val_sents_12 = drop_dict_keys(val_sents_12, features_remove_12)
result_12 = get_crf_results(train_sents_12, val_sents_12, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix',
features_remove_13 = ('is_all_upper','is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_13 = copy.deepcopy(train_sents)
val_sents_13 = copy.deepcopy(val_sents)
train_sents_13 = drop_dict_keys(train_sents_13, features_remove_13)
val_sents_13 = drop_dict_keys(val_sents_13, features_remove_13)
result_13 = get_crf_results(train_sents_13, val_sents_13, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper',
features_remove_14 = ('is_stopword','word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_14 = copy.deepcopy(train_sents)
val_sents_14 = copy.deepcopy(val_sents)
train_sents_14 = drop_dict_keys(train_sents_14, features_remove_14)
val_sents_14 = drop_dict_keys(val_sents_14, features_remove_14)
result_14 = get_crf_results(train_sents_14, val_sents_14, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',
features_remove_15 = ('word_previous', 'word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_15 = copy.deepcopy(train_sents)
val_sents_15 = copy.deepcopy(val_sents)
train_sents_15 = drop_dict_keys(train_sents_15, features_remove_15)
val_sents_15 = drop_dict_keys(val_sents_15, features_remove_15)
result_15 = get_crf_results(train_sents_15, val_sents_15, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',
# 'word_previous', 
features_remove_16 = ('word_previous_shape','word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_16 = copy.deepcopy(train_sents)
val_sents_16 = copy.deepcopy(val_sents)
train_sents_16 = drop_dict_keys(train_sents_16, features_remove_16)
val_sents_16 = drop_dict_keys(val_sents_16, features_remove_16)
result_16 = get_crf_results(train_sents_16, val_sents_16, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',
# 'word_previous', 'word_previous_shape',
features_remove_17 = ('word_previous_short_shape','word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_17 = copy.deepcopy(train_sents)
val_sents_17 = copy.deepcopy(val_sents)
train_sents_17 = drop_dict_keys(train_sents_17, features_remove_17)
val_sents_17 = drop_dict_keys(val_sents_17, features_remove_17)
result_17 = get_crf_results(train_sents_17, val_sents_17, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',
# 'word_previous', 'word_previous_shape', 'word_previous_short_shape',
features_remove_18 = ('word_after','word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_18 = copy.deepcopy(train_sents)
val_sents_18 = copy.deepcopy(val_sents)
train_sents_18 = drop_dict_keys(train_sents_18, features_remove_18)
val_sents_18 = drop_dict_keys(val_sents_18, features_remove_18)
result_18 = get_crf_results(train_sents_18, val_sents_18, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',
# 'word_previous', 'word_previous_shape', 'word_previous_short_shape', 'word_after',
features_remove_19 = ('word_after_shape','word_after_short_shape','is_in_gazetteer')
train_sents_19 = copy.deepcopy(train_sents)
val_sents_19 = copy.deepcopy(val_sents)
train_sents_19 = drop_dict_keys(train_sents_19, features_remove_19)
val_sents_19 = drop_dict_keys(val_sents_19, features_remove_19)
result_19 = get_crf_results(train_sents_19, val_sents_19, train_labels, val_labels)






#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',
# 'word_previous', 'word_previous_shape', 'word_previous_short_shape', 'word_after', 'word_after_shape',
features_remove_20 = ('word_after_short_shape','is_in_gazetteer')
train_sents_20 = copy.deepcopy(train_sents)
val_sents_20 = copy.deepcopy(val_sents)
train_sents_20 = drop_dict_keys(train_sents_20, features_remove_20)
val_sents_20 = drop_dict_keys(val_sents_20, features_remove_20)
result_20 = get_crf_results(train_sents_20, val_sents_20, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',
# 'word_previous', 'word_previous_shape', 'word_previous_short_shape', 'word_after', 'word_after_shape', 'word_after_short_shape',
features_remove_21 = ('is_in_gazetteer', 'is_in_gazetteer')
train_sents_21 = copy.deepcopy(train_sents)
val_sents_21 = copy.deepcopy(val_sents)
train_sents_21 = drop_dict_keys(train_sents_21, features_remove_21)
val_sents_21 = drop_dict_keys(val_sents_21, features_remove_21)
result_21 = get_crf_results(train_sents_21, val_sents_21, train_labels, val_labels)





#'stem', 'POS_tag', 'Chunk_tag', 'is_EOS', 'is_first_letter_uppercase', 'word_shape', 'short_word_shape','has_number','has_hyphen', 'is_upper_has_digit_has_dash', 'prefix', 'suffix','is_all_upper', 'is_stopword',
# 'word_previous', 'word_previous_shape', 'word_previous_short_shape', 'word_after', 'word_after_shape', 'word_after_short_shape', 'is_in_gazetteer'
features_remove_22 = ()
train_sents_22 = copy.deepcopy(train_sents)
val_sents_22 = copy.deepcopy(val_sents)
train_sents_22 = drop_dict_keys(train_sents_22, features_remove_22)
val_sents_22 = drop_dict_keys(val_sents_22, features_remove_22)
result_22 = get_crf_results(train_sents_22, val_sents_22, train_labels, val_labels)

print(train_sents_1[0][0])
print(train_sents_2[0][0])
print(train_sents_3[0][0])
print(train_sents_4[0][0])
print(train_sents_5[0][0])
print(train_sents_6[0][0])
print(train_sents_7[0][0])
print(train_sents_8[0][0])
print(train_sents_9[0][0])
print(train_sents_10[0][0])
print(train_sents_11[0][0])
print(train_sents_12[0][0])
print(train_sents_13[0][0])
print(train_sents_14[0][0])
print(train_sents_15[0][0])
print(train_sents_16[0][0])
print(train_sents_17[0][0])
print(train_sents_18[0][0])
print(train_sents_19[0][0])
print(train_sents_20[0][0])
print(train_sents_21[0][0])
print(train_sents_22[0][0])

# start from the stem of the token and add features one by one and train a new model with each feature that you add


features_for_add_1by1 = ['Stem', 
            '+POS_tag', 
            '+Chunk_tag', 
            '+is_BOS', 
            '+is_EOS', 
            '+is_first_letter_uppercase', 
            '+word_shape', 
            '+short_word_shape',
            '+has_number',
            '+has_hyphen',
            '+is_upper_has_digit_has_dash',
            '+prefix',
            '+suffix',
            '+is_all_upper',
            '+is_stopword',
            '+word_previous',
            '+word_previous_shape',
            '+word_previous_short_shape',
            '+word_after',
            '+word_after_shape',
            '+word_after_short_shape',
            '+is_in_gazetteer']



results_features_1by1_df = pd.DataFrame(columns = ["Precision", "Recall", "F1 Score"])
series_1 = pd.Series(result_1, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_1, ignore_index=True)

series_2 = pd.Series(result_2, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_2, ignore_index=True)

series_3 = pd.Series(result_3, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_3, ignore_index=True)

series_4 = pd.Series(result_4, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_4, ignore_index=True)

series_5 = pd.Series(result_5, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_5, ignore_index=True)

series_6 = pd.Series(result_6, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_6, ignore_index=True)

series_7 = pd.Series(result_7, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_7, ignore_index=True)

series_8 = pd.Series(result_8, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_8, ignore_index=True)

series_9 = pd.Series(result_9, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_9, ignore_index=True)

series_10 = pd.Series(result_10, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_10, ignore_index=True)

series_11 = pd.Series(result_11, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_11, ignore_index=True)

series_12 = pd.Series(result_12, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_12, ignore_index=True)

series_13 = pd.Series(result_13, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_13, ignore_index=True)

series_14 = pd.Series(result_14, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_14, ignore_index=True)

series_15 = pd.Series(result_15, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_15, ignore_index=True)

series_16 = pd.Series(result_16, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_16, ignore_index=True)

series_17 = pd.Series(result_17, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_17, ignore_index=True)

series_18 = pd.Series(result_18, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_18, ignore_index=True)

series_19 = pd.Series(result_19, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_19, ignore_index=True)

series_20 = pd.Series(result_20, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_20, ignore_index=True)

series_21 = pd.Series(result_21, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_21, ignore_index=True)

series_22 = pd.Series(result_22, index = results_features_1by1_df.columns)
results_features_1by1_df = results_features_1by1_df.append(series_22, ignore_index=True)

"""###Results Table of the Effect of Each Feature to the CRF Model"""

# display the result table
results_features_1by1_df.insert(0, "Features", features_for_add_1by1)
results_features_1by1_df

# Average Precision - Recall - F1 Score for Features
avg_result = 0
feature = ""
features_avg = []
feature_names = []
for i in range(results_features_1by1_df.shape[0]):
  for j in range(results_features_1by1_df.shape[1]):
    if j == 0:
      feature = results_features_1by1_df.loc[i][0]
      feature_names.append(feature)
    if j > 0:
      avg_result += results_features_1by1_df.loc[i][j]
  avg_result = avg_result/3
  print("Feature ["+ feature + "] overall result: ", avg_result)
  
  features_avg.append(avg_result)
  avg_result = 0

# Contribution of each feature after added to the model
feature_diffs = []
features_avg.insert(0, 0)
for i in range(1,len(features_avg)):
    x = features_avg[i] - features_avg[i-1]
    feature_diffs.append(round(x,4))
features_avg.pop(0)
features_effect_to_score = list(zip(feature_names, feature_diffs))
print("Contribution of features when added to the model:")
print(features_effect_to_score)

# Sort contributions of each features score
contribution_of_scores = copy.deepcopy(features_effect_to_score)
contribution_of_scores = sorted(contribution_of_scores, key=lambda tup: tup[1], reverse=True)

print("Contribution of features [Sorted in descending order]")
contribution_of_scores

"""###CLassification Report of the Best Model"""

# Displaying the classification report for the best model
features_remove_for_best_model = ('has_hyphen', 'word_previous_short_shape', 'is_in_gazetteer','is_upper_has_digit_has_dash', 'is_stopword', 'short_word_shape', 'is_BOS')
train_sents_for_best_model = copy.deepcopy(train_sents)
val_sents_for_best_model = copy.deepcopy(val_sents)
test_sents_for_best_model = copy.deepcopy(test_sents)

train_sents_for_best_model = drop_dict_keys(train_sents_for_best_model, features_remove_for_best_model)
val_sents_for_best_model = drop_dict_keys(val_sents_for_best_model, features_remove_for_best_model)
test_sents_for_best_model = drop_dict_keys(test_sents_for_best_model, features_remove_for_best_model)

overall_train_sents_model = train_sents_for_best_model + val_sents_for_best_model
overall_train_labels_model = train_labels + val_labels



crf_best_model_estimator = sklearn_crfsuite.CRF(c1=0.05, c2=0.25, keep_tempfiles=None)
crf_best_model_estimator.fit(overall_train_sents_model, overall_train_labels_model)
crf_best_model_estimator_preds = crf_best_model_estimator.predict(test_sents_for_best_model)

precisions_crf_best_model = sklearn_crfsuite.metrics.flat_precision_score(test_labels, crf_best_model_estimator_preds, average = 'weighted')
recalls_crf_best_model = sklearn_crfsuite.metrics.flat_recall_score(test_labels, crf_best_model_estimator_preds, average = 'weighted')
f1_scores_crf_best_model = sklearn_crfsuite.metrics.flat_f1_score(test_labels, crf_best_model_estimator_preds, average = 'weighted')



print("\nResults of the Best Model: "+ "\n"+
      "--------------------------------------------")
print("Precision:", precisions_crf_best_model)
print("Recall: ", recalls_crf_best_model)
print("F1 Score: ", f1_scores_crf_best_model)

"""## Recurrent Neural Network (RNN)

###Libraries
"""

import tensorflow as tf
import keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from keras.models import Model, Input, Sequential
from keras.layers import Dense, Flatten, Embedding, Input, Dropout, LSTM, TimeDistributed, Bidirectional, SimpleRNN
from keras.callbacks import ModelCheckpoint

from gensim.models import Word2Vec
import gensim.downloader as api
import copy

!pip install seqeval
from seqeval.metrics import f1_score, precision_score, recall_score, classification_report

"""###Prepare the Datasets by DeepCopy"""

# Prepare DeepCopies of train_sents, train_labels, val_sent, val_labels (just in case)
train_sents_rnn = copy.deepcopy(train_sents)
train_labels_rnn = copy.deepcopy(train_labels)

val_sents_rnn = copy.deepcopy(val_sents)
val_labels_rnn = copy.deepcopy(val_labels)

test_sents_rnn = copy.deepcopy(test_sents)
test_labels_rnn = copy.deepcopy(test_labels)

"""###Find MAX_LENGTH's for Padding"""

# Find Word and Label with Max Length
def FIND_MAX_LENGTH_WORD(data_sents):
  MAX_SENT_LENGTH = 0
  for sentence in data_sents:
    if len(sentence) > MAX_SENT_LENGTH:
      MAX_SENT_LENGTH = len(sentence)

  return MAX_SENT_LENGTH



def FIND_MAX_LENGTH_LABEL(data_labels):
  MAX_LENGTH_LABEL = 0
  for sentence in data_labels:
    if len(sentence) > MAX_LENGTH_LABEL:
      MAX_LENGTH_LABEL = len(sentence)

  return MAX_LENGTH_LABEL



MAX_LENGTH_train_sents = FIND_MAX_LENGTH_WORD(train_sents_rnn)
MAX_LENGTH_train_labels = FIND_MAX_LENGTH_LABEL(train_labels_rnn)

MAX_LENGTH_val_sents = FIND_MAX_LENGTH_WORD(val_sents_rnn)
MAX_LENGTH_val_labels = FIND_MAX_LENGTH_LABEL(val_labels_rnn)

MAX_LENGTH_test_sents = FIND_MAX_LENGTH_WORD(test_sents_rnn)
MAX_LENGTH_test_labels = FIND_MAX_LENGTH_LABEL(test_labels_rnn)

MAX_LENGTH_all_sents = max(MAX_LENGTH_train_sents, MAX_LENGTH_val_sents, MAX_LENGTH_test_sents)
MAX_LENGTH_all_labels = max(MAX_LENGTH_train_labels, MAX_LENGTH_val_labels, MAX_LENGTH_test_labels)

MAX_LENGTH_overall = max(MAX_LENGTH_all_sents, MAX_LENGTH_all_labels)

print("Length of the Longest Sentences:", MAX_LENGTH_all_sents)
print("Length of the Longest Label:", MAX_LENGTH_all_labels)
print("Max Length:", MAX_LENGTH_overall)

"""###Find Unique Labels"""

# find unique labels and create dictionary to map each label to a unique integer value
UniqueLabels = {}

for sent in train_labels_rnn:
  for i in sent: 
    UniqueLabels[i] = ""

for sent in val_labels_rnn:
  for i in sent: 
    UniqueLabels[i] = ""

for sent in test_labels_rnn:
  for i in sent: 
    UniqueLabels[i] = ""

print("Unique labels in the dictionary: ", UniqueLabels.keys())

"""###Map Labels to Integers"""

# Function for mapping values; 5 --> 4, 4 --> 3, 3 --> 2, 2 --> 1, 1 --> 0
labels_map = {'O': 1, 'B-ORG': 2, 'B-MISC': 3, 'B-PER': 4, 'I-PER': 5, 'B-LOC': 6, 'I-ORG': 7, 'I-MISC': 8, 'I-LOC': 9}

labels_to_list_map = {
  1: [1,0,0,0,0,0,0,0,0], 2: [0,1,0,0,0,0,0,0,0], 3: [0,0,1,0,0,0,0,0,0], 4: [0,0,0,1,0,0,0,0,0],
  5: [0,0,0,0,1,0,0,0,0], 6: [0,0,0,0,0,1,0,0,0], 7: [0,0,0,0,0,0,1,0,0], 8: [0,0,0,0,0,0,0,1,0],
  9: [0,0,0,0,0,0,0,0,1], 0: [0,0,0,0,0,0,0,0,0]
}

# Turn labels into numeric format
# Reference: https://stackoverflow.com/questions/69411748/map-elements-of-a-list-of-lists-to-a-dictionary-value
def DEEP_MAP(func, dataset_):
    if not isinstance(dataset_, list):
        return func(dataset_)
    return [DEEP_MAP(func, x) for x in dataset_]

train_labels_rnn = DEEP_MAP(lambda x: labels_map[x], train_labels_rnn)
val_labels_rnn = DEEP_MAP(lambda x: labels_map[x], val_labels_rnn)
test_labels_rnn = DEEP_MAP(lambda x: labels_map[x], test_labels_rnn)

train_labels_rnn = DEEP_MAP(lambda x: labels_to_list_map[x], train_labels_rnn)
val_labels_rnn = DEEP_MAP(lambda x: labels_to_list_map[x], val_labels_rnn)
#test_labels_rnn = DEEP_MAP(lambda x: labels_to_list_map[x], test_labels_rnn)


print("Train Labels After Mapping:\n", train_labels_rnn[:3])
print("Validation Labels After Mapping:\n", val_labels_rnn[:3])
print("Test Labels After Mapping:\n", test_labels_rnn[:3])

"""###Turn Dataset into List of List Format"""

# Turn datasets into list of list format instead of list of list of dict
# Just apply this on sents, since labels are already in list of lists format 
def LIST_OF_LIST_SENTS(data_sents):
  temp_paragraph = []
  temp_sent = []
  for sentence in data_sents:
    for word in sentence:
      stem = word.get('stem')
      temp_sent.append(stem)
    temp_paragraph.append(temp_sent)
    temp_sent = []

  return temp_paragraph

# train_sents
train_sents_rnn_lol = LIST_OF_LIST_SENTS(train_sents_rnn)

# val_sents
val_sents_rnn_lol = LIST_OF_LIST_SENTS(val_sents_rnn)

# test_sents
test_sents_rnn_lol = LIST_OF_LIST_SENTS(test_sents_rnn)

"""###Tokenize the Sentences"""

# Tokenizing sentences and labels
tokenizer_keras_rnn = Tokenizer(filters='!"$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n\'')

# train_sents
tokenizer_keras_rnn.fit_on_texts(train_sents_rnn_lol)
train_sents_rnn_tok = tokenizer_keras_rnn.texts_to_sequences(train_sents_rnn_lol)

# val_sents
tokenizer_keras_rnn.fit_on_texts(val_sents_rnn_lol)
val_sents_rnn_tok = tokenizer_keras_rnn.texts_to_sequences(val_sents_rnn_lol)

# test_sents
tokenizer_keras_rnn.fit_on_texts(test_sents_rnn_lol)
test_sents_rnn_tok = tokenizer_keras_rnn.texts_to_sequences(test_sents_rnn_lol)

print("Tokenized Train Sents:", train_sents_rnn_tok[:3])
print("Tokenized Validation Sents:", val_sents_rnn_tok[:3])
print("Tokenized Test Sents:", test_sents_rnn_tok[:3])

"""###Padding the Labels and Sentences"""

# preprare your dataset for RNN classifier (you need to add padding to labels as well)
# Padding sentences and labels

# train_sents
padded_train_sents_rnn = pad_sequences(train_sents_rnn_tok, padding='post', maxlen=MAX_LENGTH_overall)

# train_labels
padded_train_labels_rnn = pad_sequences(train_labels_rnn, padding='post', maxlen=MAX_LENGTH_overall)

#--------------------------------------------------------------------------------------------------------------------------------------------------------------

# val_sents
padded_val_sents_rnn = pad_sequences(val_sents_rnn_tok, padding='post', maxlen=MAX_LENGTH_overall)

# val_labels
padded_val_labels_rnn = pad_sequences(val_labels_rnn, padding='post', maxlen=MAX_LENGTH_overall)

#--------------------------------------------------------------------------------------------------------------------------------------------------------------

# test_sents
padded_test_sents_rnn = pad_sequences(test_sents_rnn_tok, padding='post', maxlen=MAX_LENGTH_overall)

# test_labels
padded_test_labels_rnn = pad_sequences(test_labels_rnn, padding='post', maxlen=MAX_LENGTH_overall)


print("Padded Train Sents:\n", padded_train_sents_rnn[:1], "\n")
print("Padded Train Labels:\n", padded_train_labels_rnn[:1], "\n")
print("-------------------------------------------------------------------------")
print("Padded Validation Sents:\n", padded_val_sents_rnn[:1], "\n")
print("Padded Validation Labels:\n", padded_val_labels_rnn[:1], "\n")
print("-------------------------------------------------------------------------")
print("Padded Test Sents:\n", padded_test_sents_rnn[:1], "\n")
print("Padded Test Labels:\n", padded_test_labels_rnn[:1])

padded_train_sents_rnn.shape

"""###Create the Embedding Layers"""

# RANDOMLY INITIALIZED
random_emb_layer_rnn = Embedding(input_dim=len(tokenizer_keras_rnn.word_index)+1, output_dim=64, input_length=MAX_LENGTH_overall)

# FROM SCRATCH
my_rnn_embedding = Word2Vec(sentences = train_sents_rnn_lol, min_count=1)
my_rnn_emb_layer = Embedding(input_dim=len(my_rnn_embedding.wv.vocab), output_dim=100, input_length=MAX_LENGTH_overall, weights=[my_rnn_embedding.wv.vectors])

# Create your own word embeddings from scratch and load a pretrained word embeddings
# You can check https://radimrehurek.com/gensim/models/word2vec.html for training a word embeddings from scratch
# You can check https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html and https://github.com/RaRe-Technologies/gensim-data for loading pretrained word embeddings.

# PRETRAINED
pretrained_rnn_embedding = Word2Vec(sentences=api.load('text8'), min_count=1)
pretrained_rnn_emb_layer = Embedding(input_dim=len(pretrained_rnn_embedding.wv.vocab), output_dim=100, input_length=MAX_LENGTH_overall, weights=[pretrained_rnn_embedding.wv.vectors])

"""###Create the RNN Models"""

# Turn labels into numeric format
# Reference: https://stackoverflow.com/questions/69411748/map-elements-of-a-list-of-lists-to-a-dictionary-value
def DEEP_MAP(func, dataset_):
    if not isinstance(dataset_, list):
        return func(dataset_)
    return [DEEP_MAP(func, x) for x in dataset_]

# decode a one hot encoded string
def one_hot_decode(encoded_seq):
  d = []
  for label_list in encoded_seq[0]:
    e = []
    for vector in label_list:
      e.append(np.argmax(vector) + 1)
    d.append(e)
  return d
# define a function to remove paddings and align labels and tokens

def reverse_map(sequence):
  # Function for mapping values; 5 --> 4, 4 --> 3, 3 --> 2, 2 --> 1, 1 --> 0
  reverse_labels_map = {1: 'O', 2: 'B-ORG', 3: 'B-MISC', 4: 'B-PER', 5: 'I-PER', 6: 'B-LOC', 7: 'I-ORG', 8: 'I-MISC', 9:'I-LOC'}

  sequence =  DEEP_MAP(lambda x: reverse_labels_map[x], sequence)
  return sequence

"""####SimpleRNN Model"""

def RNN_Model_Generator(op_name, embedding_layer_opt, rnn_layer_size, hidden_layer_size, 
                            padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn):

    print(op_name)
    print("=================================")
    print("RNN Hidden Layer Size: ", rnn_layer_size)
    print("Hidden Layer Size: ", hidden_layer_size)
    print("--------------------------------------------------------------------- \n")


    # Define the Model
    # --------------------------------------------------------------------------
    model = Sequential()

    # Add Embedding Layer
    model.add(embedding_layer_opt)

    # Add RNN
    model.add(SimpleRNN(units=rnn_layer_size, return_sequences=True))

    model.add(Dropout(rate=0.02))

    # Add Dense Layer
    model.add(Dense(hidden_layer_size, activation='relu'))

    # Add timeDistributed Layer
    model.add(TimeDistributed(Dense(9, activation="softmax")))

    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    
    print("\n--------------------------------------------------------------------- \n")

    model.fit(padded_train_sents_rnn, padded_train_labels_rnn, epochs=10, verbose=1)
    pred_labels = model.predict(padded_val_sents_rnn)

    pred_labels_new = one_hot_decode(pred_labels)
    pred_labels_newest = reverse_map(pred_labels_new)

    padded_val_labels_rnn_new = one_hot_decode(padded_val_labels_rnn)
    padded_val_labels_rnn_newest = reverse_map(padded_val_labels_rnn_new)
    print("F1 Score: {:.4%}".format(f1_score(padded_val_labels_rnn_newest, pred_labels_newest)))
    print("Precision Score: {:.4%}".format(precision_score(padded_val_labels_rnn_newest, pred_labels_newest)))
    print("Recall Score: {:.4%}".format(recall_score(padded_val_labels_rnn_newest, pred_labels_newest)))
    print("\n\n\n")

"""####LSTM Model"""

def LSTM_Model_Generator(op_name, embedding_layer_opt, rnn_layer_size, hidden_layer_size, 
                            padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn):
  
    print(op_name)
    print("=================================")
    print("RNN Hidden Layer Size: ", rnn_layer_size)
    print("Hidden Layer Size: ", hidden_layer_size)
    print("--------------------------------------------------------------------- \n")

    model = Sequential()

    # Add Embedding layer
    model.add(embedding_layer_opt)

    # Add LSTM
    model.add
    model.add(LSTM(units=rnn_layer_size, return_sequences=True))

    model.add(Dropout(rate=0.02))

    model.add(Dense(hidden_layer_size, activation='relu'))

    # Add timeDistributed Layer
    model.add(TimeDistributed(Dense(9, activation="softmax")))

    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    #model.summary()
    model.fit(padded_train_sents_rnn, padded_train_labels_rnn, epochs=10, verbose=1)
    pred_labels = model.predict(padded_val_sents_rnn)

    pred_labels_new = one_hot_decode(pred_labels)
    pred_labels_newest = reverse_map(pred_labels_new)

    padded_val_labels_rnn_new = one_hot_decode(padded_val_labels_rnn)
    padded_val_labels_rnn_newest = reverse_map(padded_val_labels_rnn_new)
    print("F1 Score: {:.4%}".format(f1_score(padded_val_labels_rnn_newest, pred_labels_newest)))
    print("Precision Score: {:.4%}".format(precision_score(padded_val_labels_rnn_newest, pred_labels_newest)))
    print("Recall Score: {:.4%}".format(recall_score(padded_val_labels_rnn_newest, pred_labels_newest)))
    print("\n\n\n")

"""####Bidirectional LSTM Model"""

# Create your models and train them
# RNN Model Reference: https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54

def Bi_LSTM_Model_Generator(op_name, embedding_layer_opt, rnn_layer_size, hidden_layer_size, 
                            padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn):
  
    print(op_name)
    print("=================================")
    print("RNN Hidden Layer Size: ", rnn_layer_size)
    print("Hidden Layer Size: ", hidden_layer_size)
    print("--------------------------------------------------------------------- \n")

    model = Sequential()
    # Add Embedding layer
    model.add(embedding_layer_opt)

    # Add bidirectional LSTM
    model.add(Bidirectional(LSTM(units=rnn_layer_size, return_sequences=True)))

    model.add(Dropout(rate=0.02))

    model.add(Dense(hidden_layer_size, activation='relu'))

    # Add timeDistributed Layer
    model.add(TimeDistributed(Dense(9, activation="softmax")))

    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
  
    model.fit(padded_train_sents_rnn, padded_train_labels_rnn, epochs=10, verbose=1)
    pred_labels = model.predict(padded_val_sents_rnn)

    pred_labels_new = one_hot_decode(pred_labels)
    pred_labels_newest = reverse_map(pred_labels_new)

    padded_val_labels_rnn_new = one_hot_decode(padded_val_labels_rnn)
    padded_val_labels_rnn_newest = reverse_map(padded_val_labels_rnn_new)
    print("F1 Score: {:.4%}".format(f1_score(padded_val_labels_rnn_newest, pred_labels_newest)))
    print("Precision Score: {:.4%}".format(precision_score(padded_val_labels_rnn_newest, pred_labels_newest)))
    print("Recall Score: {:.4%}".format(recall_score(padded_val_labels_rnn_newest, pred_labels_newest)))
    print("\n\n\n")



"""###Generate the Models"""

import numpy as np

# General
operation_names = ['Random Embedded Model', 'Embedded From Scratch Model', 'Pretrained Embedding Model']
embedding_layer_options = [random_emb_layer_rnn, my_rnn_emb_layer, pretrained_rnn_emb_layer]
RNN_layer_size_options = [32, 64]
hidden_layer_options =[16, 24]

# Random Embedded Model
for a in range(len(RNN_layer_size_options)):
  for b in range(len(hidden_layer_options)):
    RNN_Model_Generator('Random Embedded Model', random_emb_layer_rnn, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn)
    LSTM_Model_Generator('Random Embedded Model', random_emb_layer_rnn, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn)
    Bi_LSTM_Model_Generator('Random Embedded Model', random_emb_layer_rnn, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn)

# Embedded From Scratch Model
for a in range(len(RNN_layer_size_options)):
  for b in range(len(hidden_layer_options)):
    RNN_Model_Generator('Embedded From Scratch Model', my_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn)
    LSTM_Model_Generator('Embedded From Scratch Model', my_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn)
    Bi_LSTM_Model_Generator('Embedded From Scratch Model', my_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn)

# Pretrained Embedding Model
for a in range(len(RNN_layer_size_options)):
  for b in range(len(hidden_layer_options)):
    RNN_Model_Generator('Pretrained Embedding Model', pretrained_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn)
    LSTM_Model_Generator('Pretrained Embedding Model', pretrained_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn)
    Bi_LSTM_Model_Generator('Pretrained Embedding Model', pretrained_rnn_emb_layer, RNN_layer_size_options[a], hidden_layer_options[b], padded_train_sents_rnn, padded_train_labels_rnn, 
                            padded_val_sents_rnn, padded_val_labels_rnn, 
                            padded_test_sents_rnn, padded_test_labels_rnn)

padded_overall_sents = np.concatenate((padded_train_sents_rnn, padded_val_sents_rnn), axis=0)
padded_overall_labels = np.concatenate((padded_train_labels_rnn, padded_val_labels_rnn), axis=0)

# Evaluate your models with functions of seqeval library
# Since every model generates the same results, it is useless to make evaluation on test data using (train + validation) data.

"""## My Report

Named Entity Recognition task carried out best with CRF model with very high F1, Precision, and Recall Score (96-97%). Feature extraction step and eliminating the features which has no contribution or the features which decreases the score of the model, improves the overall score of the model in the test data. 

In the RNN part of the project, due to an "overfitting" situation experienced with model; the F1, Precision, and Recall Scores are very very low. I think this situation is caused by the zero's in the data due to padding. Due to the majority of zero's in the data, RNN models assummed that, marking every word with "zero" which corresponds to the "Other" label provide the best predictions. This conclusion is derived from the epochs. In the epochs, accuracy of the model's are displayed around 93 - 96%. But, when the time for testing the model on the validation set comes, the model always predict the label as zero.

An analogy about this situation can be formed as the following:
If we provide 90 cat images and 10 dog images to a model and expect model to predict the images we give as the test set, the model will predict every image as "cat" in order to work with 90% accuracy.

As the result of these experiments it can ben concluded that, CRF models work best for NER task and RNN - LSTM models are not useful for this task.

##NOTE ABOUT SUBMISSION: 
I submitted this homework 2 days late because in the first day I was trying to fix some errors and in the 2nd day even if I start running the code in the afternoon, at the middle of the execution somehow my dorm internet get disconnected. Because of that I re-run the code but the code couldn't finish the execution unyil the midnight. Today, I couldn't run the models with "Embedded From Scratch Model" and "Pretrained Embedding Model" because runtime is disconnected and my GPU limit is finished when I was having my CS412 Machine Learning Final.
"""